---
marp: true
theme: default
paginate: true
header: 'AI 기초체력훈련 | 19차시'
footer: '© 2026 AI 기초체력훈련'
style: |
  section { font-family: 'Malgun Gothic', sans-serif; }
  h1 { color: #2563eb; }
  h2 { color: #1e40af; }
  code { background-color: #f1f5f9; }
---

# 딥러닝 입문: 신경망 기초

## 19차시 | AI 기초체력훈련 (Pre AI-Campus)

**인공지능의 핵심, 신경망 이해하기**

---

# 학습목표

이 차시를 마치면 다음을 할 수 있습니다:

1. **신경망**의 기본 구조를 이해한다
2. **뉴런과 층**의 개념을 설명한다
3. **딥러닝 vs 머신러닝**의 차이를 구분한다

---

# 딥러닝이란?

## Deep Learning

> 여러 층의 **신경망**으로 학습하는 머신러닝의 한 분야

```
  머신러닝
     │
     ├── 선형회귀
     ├── 의사결정트리
     ├── 랜덤포레스트
     └── 딥러닝 ← 여러 층의 신경망
            ├── MLP
            ├── CNN (이미지)
            └── RNN (시계열)
```

---

# 왜 딥러닝인가?

## 복잡한 패턴 학습

### 전통적 ML
- 특성 엔지니어링이 중요
- 사람이 특성을 설계

### 딥러닝
- 스스로 특성을 학습
- 복잡한 패턴 발견 가능
- 이미지, 텍스트, 음성에서 강력

> 데이터가 많고 복잡한 문제에 효과적!

---

# 생물학적 신경망

## 뇌의 뉴런

```
    수상돌기            세포체           축삭돌기
  (입력 받음)       (처리/판단)       (출력 전달)

      ○──┐             ┌─○─┐
      ○──┼───────●────●     ●───→ 다음 뉴런으로
      ○──┘        ↑
                신경전달물질
```

- 여러 뉴런에서 신호 수신
- 임계값 초과 시 활성화
- 다음 뉴런으로 전달

---

# 인공 뉴런

## Artificial Neuron

```
  입력                   가중치           활성화 함수        출력

   x₁ ──→ ● ×w₁ ─┐
                  │
   x₂ ──→ ● ×w₂ ─┼──→ Σ + b ──→ f(x) ──→ y
                  │
   x₃ ──→ ● ×w₃ ─┘

        입력 × 가중치      합산     비선형 변환    최종 출력
```

$$y = f(w_1x_1 + w_2x_2 + w_3x_3 + b)$$

---

# 가중치와 편향

## Weights & Bias

```python
# 입력
x = [85, 50, 100]  # 온도, 습도, 속도

# 가중치 (학습되는 값)
w = [0.3, 0.2, 0.5]  # 각 입력의 중요도

# 편향
b = 0.1

# 계산
z = 85*0.3 + 50*0.2 + 100*0.5 + 0.1
# z = 25.5 + 10 + 50 + 0.1 = 85.6
```

> 가중치 = 입력의 중요도
> 편향 = 기준점 조정

---

# 활성화 함수

## Activation Function

> 비선형성을 추가하는 함수

### 주요 활성화 함수

| 함수 | 범위 | 특징 |
|------|------|------|
| Sigmoid | 0~1 | 확률 출력에 적합 |
| ReLU | 0~∞ | 현재 가장 많이 사용 |
| Tanh | -1~1 | 중심이 0 |

---

# ReLU 함수

## Rectified Linear Unit

```
f(x) = max(0, x)

         │
       ↗│
      /  │
     /   │
────/────┼────────→ x
         │
         │
```

- 음수 → 0
- 양수 → 그대로

```python
import numpy as np
relu = lambda x: np.maximum(0, x)
```

---

# Sigmoid 함수

## 0과 1 사이로 압축

```
f(x) = 1 / (1 + e^(-x))

     1 ─ ─ ─ ─ ─ ╭───────
               ╱
             ╱
   0.5 ────╋────────────→ x
         ╱
       ╱
     0 ───────╯─ ─ ─ ─ ─
```

- 확률 출력에 적합
- 이진 분류의 마지막 층

---

# 신경망의 구조

## 층 (Layer)

```
  입력층          은닉층           출력층
 (Input)       (Hidden)        (Output)

   ○              ○
   ○     →      ○ ○      →       ○
   ○              ○               ○
   ○              ○

특성 입력      특성 추출        최종 예측
```

> **은닉층이 많으면 = 깊은(Deep) 신경망**

---

# 다층 퍼셉트론 (MLP)

## Multi-Layer Perceptron

```
      입력층       은닉층1      은닉층2       출력층

        ○          ○            ○
        ○    →    ○ ○    →     ○ ○    →     ○
        ○          ○            ○
        ○

     (3개)       (4개)        (3개)        (1개)
```

- 가장 기본적인 신경망
- **Fully Connected**: 모든 뉴런이 연결

---

# 학습 과정

## 순전파와 역전파

```
1. 순전파 (Forward)
   입력 → 예측값 계산

2. 손실 계산 (Loss)
   예측값 vs 실제값 비교

3. 역전파 (Backward)
   가중치 조정 방향 계산

4. 가중치 업데이트
   경사하강법으로 조정

→ 1~4 반복 (에폭)
```

---

# 손실 함수

## Loss Function

> 예측이 얼마나 틀렸는지 측정

### 회귀: MSE
$$Loss = \frac{1}{n}\sum(y_{실제} - y_{예측})^2$$

### 분류: Cross Entropy
$$Loss = -\sum y_{실제} \cdot \log(y_{예측})$$

> 손실을 **최소화**하는 방향으로 학습

---

# 경사하강법

## Gradient Descent

```
   Loss
    │  ●
    │   \
    │    \       현재 위치에서 기울기(경사) 계산
    │     \      → 아래 방향으로 이동
    │      \
    │       ●
    │        \___●  최소점 도달
    └───────────────→ 가중치
```

$$w_{new} = w_{old} - \eta \cdot \frac{\partial Loss}{\partial w}$$

> η (에타) = 학습률

---

# 학습률 (Learning Rate)

## 너무 크면? 너무 작으면?

```
  학습률 너무 큼           적절한 학습률          학습률 너무 작음

     ●    ●              ●                       ●
      \  /                \                       \
       \/                  \                       \
       /\                   ●                       ●
      ●  ●                   \●                      \●
                                                       \●
   발산!               빠른 수렴              너무 느린 수렴
```

---

# 딥러닝 vs 머신러닝

## 언제 무엇을?

| | 머신러닝 | 딥러닝 |
|--|---------|--------|
| 데이터 양 | 적어도 가능 | 많을수록 좋음 |
| 특성 추출 | 직접 설계 | 자동 학습 |
| 해석 | 비교적 쉬움 | 블랙박스 |
| 학습 시간 | 빠름 | 오래 걸림 |
| 적합한 문제 | 테이블 데이터 | 이미지, 텍스트 |

---

# 딥러닝 프레임워크

## Python 라이브러리

### TensorFlow / Keras
```python
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense
```

### PyTorch
```python
import torch
import torch.nn as nn
```

> 이 과정에서는 **Keras**를 사용합니다

---

# 정리

## 핵심 개념

| 개념 | 설명 |
|------|------|
| 뉴런 | 입력 × 가중치 + 편향 → 활성화 |
| 층 (Layer) | 뉴런의 집합 |
| 활성화 함수 | ReLU, Sigmoid 등 비선형 변환 |
| 손실 함수 | 예측 오차 측정 |
| 경사하강법 | 손실 최소화 방향으로 가중치 조정 |

---

# 다음 차시 예고

## 20차시: 딥러닝 실습

- Keras로 MLP 구현
- 제조 데이터 품질 예측
- 모델 학습과 평가

> 직접 신경망을 만들어봅니다!

---

# 감사합니다

## AI 기초체력훈련 19차시

**딥러닝 입문: 신경망 기초**

신경망의 원리를 이해했습니다!
