# [11차시] 분류 모델 (1): 의사결정나무 - 강사 스크립트

## 강의 정보
- **차시**: 11차시 (25-30분)
- **유형**: 이론 + 실습
- **구성**: 이론 10분 + 실습 15-20분
- **대상**: 비전공자, AI 입문자, 제조업 종사자

---

## 이론편 (10분)

### 도입 (2분)

#### 인사 및 지난 시간 복습 [1분]

> 안녕하세요, 11차시를 시작하겠습니다.
>
> 지난 시간에 머신러닝의 개념과 분류/회귀를 배웠죠? 오늘은 드디어 첫 번째 AI 모델을 본격적으로 만들어봅니다.
>
> 바로 **의사결정나무(Decision Tree)**입니다.

#### 학습목표 안내 [1분]

> 오늘 수업을 마치면 다음 세 가지를 할 수 있습니다.
>
> 첫째, 의사결정나무의 원리를 설명합니다.
> 둘째, DecisionTreeClassifier로 분류 모델을 구축합니다.
> 셋째, 모델의 예측과 성능을 평가합니다.
>
> 오늘 배우는 패턴이 앞으로 모든 모델에 적용됩니다.

---

### 핵심 내용 (8분)

#### 의사결정나무란? [2분]

> **의사결정나무**는 질문을 통해 단계적으로 분류하는 알고리즘입니다.
>
> 스무고개 게임을 생각해보세요. "동물인가요?", "다리가 4개인가요?" 이런 식으로 질문해서 정답을 맞추죠?
>
> 의사결정나무도 똑같아요.
>
> "온도가 88도 넘나요?" → 예 → "습도가 60% 넘나요?" → 예 → 불량
>
> 이런 식으로 질문을 따라가면서 분류합니다.
>
> 제조 현장에서 현장 전문가들이 경험적으로 하는 판단이 있잖아요. "온도가 너무 높으면 불량이 많아" 같은 거요. 이게 바로 트리 형태로 표현되는 겁니다.

#### 트리 구성 요소 [1분]

> 트리의 구성 요소를 알아볼게요.
>
> **Root Node**는 맨 위에 있는 첫 번째 질문입니다.
>
> **Branch**는 가지예요. 질문에 대한 예/아니오 답에 따라 갈라집니다.
>
> **Leaf Node**는 맨 끝에 있는 최종 결과입니다. 여기서 "정상" 또는 "불량"이 결정돼요.

#### 어떻게 질문을 만들까? [1.5분]

> 중요한 질문이 있어요. "어떤 질문이 가장 좋은 질문일까?"
>
> 데이터를 **잘 나누는 질문**이 좋은 질문입니다.
>
> 예를 들어, "온도 > 88?"이라는 질문을 했을 때, 왼쪽에는 불량이 많고 오른쪽에는 정상이 많으면 좋은 질문이에요.
>
> 반대로 양쪽에 섞여 있으면 나쁜 질문입니다.
>
> 이걸 **정보 이득(Information Gain)**이라고 해요. 알고리즘이 자동으로 가장 좋은 질문을 찾아줍니다.

#### sklearn 사용법 [2분]

> 이제 코드로 만들어봅시다. sklearn의 DecisionTreeClassifier를 사용합니다.
>
> ```python
> from sklearn.tree import DecisionTreeClassifier
> model = DecisionTreeClassifier()
> model.fit(X_train, y_train)
> ```
>
> 세 줄이면 됩니다. 모델 만들고 fit으로 학습시키면 끝이에요.
>
> 예측할 때는 `model.predict(X_test)`. 확률을 보고 싶으면 `model.predict_proba(X_test)`.
>
> 성능 평가는 `model.score(X_test, y_test)`로 정확도를 봅니다.

#### 과대적합 주의 [1.5분]

> 중요한 주의사항이 있어요. **과대적합(Overfitting)** 문제입니다.
>
> 트리가 너무 깊어지면 학습 데이터를 외워버립니다. 외웠기 때문에 학습 정확도는 99%가 나오죠.
>
> 그런데 새로운 데이터에서는? 70%밖에 안 나와요. 이해한 게 아니라 외운 거니까요.
>
> 이걸 방지하려면 `max_depth` 파라미터로 트리 깊이를 제한합니다.
>
> 보통 3~10 정도로 설정해요. 학습 정확도와 테스트 정확도 차이가 크면 과대적합을 의심하세요.

---

## 실습편 (15-20분)

### 실습 소개 [2분]

> 이제 실습 시간입니다. 제조 데이터로 불량 분류 모델을 만들어봅니다.
>
> **실습 목표**입니다.
> 1. 온도, 습도, 속도로 불량 여부를 예측합니다.
> 2. 의사결정나무 모델을 학습시킵니다.
> 3. 성능을 평가하고 시각화합니다.
>
> **실습 환경**을 확인해주세요.
>
> ```python
> from sklearn.tree import DecisionTreeClassifier, plot_tree
> from sklearn.model_selection import train_test_split
> ```

### 실습 1: 데이터 생성 [2분]

> 첫 번째 실습입니다. 제조 데이터를 생성합니다.
>
> ```python
> np.random.seed(42)
> n = 300
>
> df = pd.DataFrame({
>     '온도': np.random.normal(85, 5, n),
>     '습도': np.random.normal(50, 10, n),
>     '속도': np.random.normal(100, 15, n),
> })
> ```
>
> 불량 여부는 온도와 습도가 높을수록 확률이 높게 설정했습니다.
>
> 실제 현장 데이터와 비슷한 패턴이에요.

### 실습 2: 데이터 분리 [2분]

> 두 번째 실습입니다. 특성과 타겟을 분리하고, 학습/테스트로 나눕니다.
>
> ```python
> X = df[['온도', '습도', '속도']]
> y = df['불량여부']
>
> X_train, X_test, y_train, y_test = train_test_split(
>     X, y, test_size=0.2, random_state=42, stratify=y
> )
> ```
>
> stratify=y를 넣으면 학습/테스트에 불량 비율이 비슷하게 유지됩니다.

### 실습 3: 모델 학습 [2분]

> 세 번째 실습입니다. 모델을 만들고 학습시킵니다.
>
> ```python
> model = DecisionTreeClassifier(max_depth=5, random_state=42)
> model.fit(X_train, y_train)
> ```
>
> max_depth=5로 깊이를 제한했어요. 과대적합 방지용입니다.
>
> 트리 깊이와 리프 노드 수도 확인해봅시다.

### 실습 4: 예측 [2분]

> 네 번째 실습입니다. 새 데이터로 예측해봅니다.
>
> ```python
> new_data = [[90, 55, 100]]
> pred = model.predict(new_data)
> proba = model.predict_proba(new_data)
>
> print(f"예측: {'불량' if pred[0]==1 else '정상'}")
> print(f"불량 확률: {proba[0][1]:.1%}")
> ```
>
> 단순히 "불량"이 아니라 "75% 확률로 불량"처럼 확률도 볼 수 있어요.
>
> 현장에서 의사결정할 때 유용합니다.

### 실습 5: 성능 평가 [2분]

> 다섯 번째 실습입니다. 성능을 평가하고 과대적합을 확인합니다.
>
> ```python
> train_acc = model.score(X_train, y_train)
> test_acc = model.score(X_test, y_test)
>
> print(f"학습 정확도: {train_acc:.1%}")
> print(f"테스트 정확도: {test_acc:.1%}")
> ```
>
> 차이가 10%p 이상이면 과대적합을 의심하세요.
>
> 그럴 때는 max_depth를 줄여보면 됩니다.

### 실습 6: 트리 시각화 [2분]

> 여섯 번째 실습입니다. 트리를 시각화해봅니다.
>
> ```python
> from sklearn.tree import plot_tree
>
> plt.figure(figsize=(20, 10))
> plot_tree(model, feature_names=['온도', '습도', '속도'],
>           class_names=['정상', '불량'], filled=True)
> plt.show()
> ```
>
> 이게 의사결정나무의 큰 장점이에요. 왜 그렇게 예측했는지 눈으로 볼 수 있습니다.
>
> "온도가 87.5도 넘으면 불량 가능성이 높아진다"는 걸 그래프로 확인할 수 있죠.

### 실습 7: 특성 중요도 [2분]

> 일곱 번째 실습입니다. 어떤 변수가 중요한지 확인합니다.
>
> ```python
> importance = pd.DataFrame({
>     '특성': X.columns,
>     '중요도': model.feature_importances_
> }).sort_values('중요도', ascending=False)
>
> print(importance)
> ```
>
> 온도가 가장 중요하다면, 온도 관리에 집중해야겠죠. 현장에 적용할 수 있는 인사이트입니다.

### 실습 8: 하이퍼파라미터 실험 [2분]

> 마지막 실습입니다. max_depth를 바꿔가며 성능 변화를 봅시다.
>
> ```python
> for depth in [1, 2, 3, 5, 10, None]:
>     model_temp = DecisionTreeClassifier(max_depth=depth)
>     model_temp.fit(X_train, y_train)
>     train_acc = model_temp.score(X_train, y_train)
>     test_acc = model_temp.score(X_test, y_test)
>     print(f"depth={depth}: 학습={train_acc:.1%}, 테스트={test_acc:.1%}")
> ```
>
> depth가 너무 크면 학습 정확도는 높지만 테스트 정확도가 떨어져요. 이게 과대적합입니다.

---

### 정리 (3분)

#### 핵심 요약 [1.5분]

> 오늘 배운 내용을 정리하겠습니다.
>
> **의사결정나무**는 질문으로 단계적으로 분류하는 알고리즘입니다.
>
> **sklearn 사용법**은:
> - model.fit(X_train, y_train): 학습
> - model.predict(X_test): 예측
> - model.score(X_test, y_test): 평가
>
> **과대적합 주의**: max_depth로 깊이를 제한하세요.
>
> **해석 가능**: plot_tree로 왜 그렇게 예측했는지 볼 수 있습니다.

#### 다음 차시 예고 [1분]

> 다음 12차시에서는 **랜덤포레스트**를 배웁니다.
>
> 의사결정나무 하나는 불안정할 수 있어요. 그래서 여러 개의 트리를 만들어서 투표하는 방식이 랜덤포레스트입니다.
>
> 숲(Forest)처럼 여러 트리를 모으면 성능이 더 좋아집니다.

#### 마무리 [0.5분]

> 오늘 첫 번째 AI 모델을 만들어봤습니다.
>
> 이 패턴을 잘 기억하세요. 앞으로 배우는 모든 모델이 같은 방식입니다.
>
> 수고하셨습니다!

---

## 강의 노트

### 준비물
- PPT 슬라이드 (slides.md)
- 실습 코드 파일 (code.py)
- matplotlib 한글 폰트 설정 확인

### 주의사항
- 과대적합 개념 충분히 설명
- max_depth 실험으로 직접 확인
- 트리 시각화로 해석 가능성 강조

### 예상 질문
1. "의사결정나무가 실무에서 많이 쓰이나요?"
   → 해석이 필요할 때 유용. 랜덤포레스트가 더 많이 쓰임

2. "max_depth를 몇으로 해야 하나요?"
   → 데이터에 따라 다름. 실험해서 테스트 정확도가 가장 높은 값 선택

3. "확률이 50%면 어떻게 판단하나요?"
   → 임계값(threshold) 조정 가능. 다음 차시에서 다룸

4. "특성 중요도가 높으면 무조건 좋은 건가요?"
   → 중요도가 높은 변수에 집중. 하지만 인과관계는 아님
