# [12차시] 분류 모델 (2): 랜덤포레스트 - 강사 스크립트

## 강의 정보
- **차시**: 12차시 (25-30분)
- **유형**: 이론 + 실습
- **구성**: 이론 10분 + 실습 15-20분
- **대상**: 비전공자, AI 입문자, 제조업 종사자

---

## 이론편 (10분)

### 도입 (2분)

#### 인사 및 지난 시간 복습 [1분]

> 안녕하세요, 12차시를 시작하겠습니다.
>
> 지난 시간에 의사결정나무를 배웠죠? 해석하기 쉽다는 장점이 있었지만, 과대적합 위험이 있고 불안정하다는 단점도 있었습니다.
>
> 오늘은 이 단점을 극복하는 **랜덤포레스트**를 배웁니다.

#### 학습목표 안내 [1분]

> 오늘 수업을 마치면 다음 세 가지를 할 수 있습니다.
>
> 첫째, 앙상블 학습의 개념을 설명합니다.
> 둘째, 랜덤포레스트의 원리를 이해합니다.
> 셋째, RandomForestClassifier로 분류 모델을 구축합니다.
>
> 랜덤포레스트는 실무에서 가장 많이 쓰이는 모델 중 하나입니다.

---

### 핵심 내용 (8분)

#### 앙상블 학습이란? [1.5분]

> **앙상블 학습**은 여러 모델을 합쳐서 더 좋은 성능을 내는 방법입니다.
>
> 비유를 들어볼게요. 중요한 결정을 할 때 한 사람 의견만 듣나요? 아니죠. 여러 전문가 의견을 듣고 종합하잖아요.
>
> 앙상블도 마찬가지입니다. 여러 모델이 각자 예측하고, 그 결과를 투표하거나 평균 내서 최종 결정을 합니다.
>
> 혼자보다 여럿이 함께 결정하면 더 정확해요. 이게 앙상블의 핵심입니다.

#### 랜덤포레스트 원리 [2분]

> **랜덤포레스트**는 이름 그대로 랜덤한 숲입니다.
>
> 의사결정나무를 100개 만들어서, 각 트리가 예측한 결과를 투표합니다.
>
> 트리 100개 중 72개가 "정상"이라고 했고, 28개가 "불량"이라고 했으면? 최종 결과는 "정상"입니다.
>
> 여기서 중요한 건 **"랜덤"**이에요. 두 가지가 랜덤입니다.
>
> 첫째, **데이터 랜덤**. 각 트리는 전체 데이터에서 랜덤하게 뽑은 샘플로 학습합니다. 복원 추출이라서 같은 데이터가 여러 번 뽑힐 수도 있어요.
>
> 둘째, **특성 랜덤**. 각 노드에서 분할할 때 모든 특성을 보는 게 아니라 일부만 랜덤하게 선택합니다.
>
> 이렇게 하면 트리마다 다 달라요. 다양한 트리가 모이면 서로 보완해서 더 정확해집니다.

#### sklearn 사용법 [2분]

> 코드로 만들어봅시다. sklearn의 RandomForestClassifier를 사용합니다.
>
> ```python
> from sklearn.ensemble import RandomForestClassifier
>
> model = RandomForestClassifier(n_estimators=100, random_state=42)
> model.fit(X_train, y_train)
> ```
>
> 의사결정나무와 거의 똑같죠? n_estimators=100이 추가됐어요. 이게 트리 개수입니다.
>
> 예측이나 평가도 똑같습니다.
> - model.predict(X_test): 예측
> - model.score(X_test, y_test): 정확도
> - model.feature_importances_: 특성 중요도

#### 핵심 파라미터 [1.5분]

> 중요한 파라미터를 알아볼게요.
>
> **n_estimators**: 트리 개수입니다. 기본값이 100이에요. 보통 100~200개면 충분합니다. 너무 많으면 학습 시간만 늘어나고 성능 향상은 미미해요.
>
> **max_depth**: 각 트리의 최대 깊이입니다. 의사결정나무에서 배웠죠? 과대적합 방지용입니다.
>
> **max_features**: 노드 분할 시 고려할 특성 개수입니다. 기본값 'sqrt'면 특성 개수의 제곱근만큼 선택해요.

#### 장단점 [1분]

> 랜덤포레스트의 장점입니다.
>
> 첫째, **높은 성능**. 대부분의 문제에서 좋은 결과를 냅니다.
> 둘째, **안정적**. 과대적합 위험이 적어요.
> 셋째, **특성 중요도**. 어떤 변수가 중요한지 알 수 있습니다.
>
> 단점도 있어요.
>
> 첫째, **해석 어려움**. 트리가 100개라서 각각을 다 볼 수 없습니다.
> 둘째, **학습 시간**. 단일 트리보다 오래 걸려요.
>
> 하지만 장점이 훨씬 크기 때문에 실무에서 많이 씁니다.

---

## 실습편 (15-20분)

### 실습 소개 [2분]

> 이제 실습 시간입니다. 랜덤포레스트로 불량 분류 모델을 만들고, 의사결정나무와 비교해봅니다.
>
> **실습 목표**입니다.
> 1. 랜덤포레스트 모델을 학습시킵니다.
> 2. 의사결정나무와 성능을 비교합니다.
> 3. 특성 중요도를 분석합니다.
>
> **실습 환경**을 확인해주세요.
>
> ```python
> from sklearn.ensemble import RandomForestClassifier
> from sklearn.tree import DecisionTreeClassifier
> ```

### 실습 1: 데이터 준비 [2분]

> 첫 번째 실습입니다. 제조 데이터를 생성합니다.
>
> 오늘은 500개로 좀 더 많이 만들어볼게요. 랜덤포레스트는 데이터가 많을수록 성능이 좋아집니다.
>
> ```python
> np.random.seed(42)
> n = 500
> df = pd.DataFrame({...})
> ```

### 실습 2: 랜덤포레스트 학습 [2분]

> 두 번째 실습입니다. 랜덤포레스트 모델을 학습시킵니다.
>
> ```python
> from sklearn.ensemble import RandomForestClassifier
>
> model = RandomForestClassifier(n_estimators=100, random_state=42)
> model.fit(X_train, y_train)
>
> print(f"정확도: {model.score(X_test, y_test):.1%}")
> ```
>
> 의사결정나무와 코드가 거의 같죠? 모델 이름과 n_estimators만 다릅니다.

### 실습 3: 성능 비교 [2분]

> 세 번째 실습입니다. 의사결정나무와 성능을 비교해봅시다.
>
> ```python
> dt = DecisionTreeClassifier(random_state=42)
> dt.fit(X_train, y_train)
>
> rf = RandomForestClassifier(n_estimators=100, random_state=42)
> rf.fit(X_train, y_train)
>
> print(f"의사결정나무: {dt.score(X_test, y_test):.1%}")
> print(f"랜덤포레스트: {rf.score(X_test, y_test):.1%}")
> ```
>
> 대부분의 경우 랜덤포레스트가 더 높게 나옵니다.

### 실습 4: 특성 중요도 [2min]

> 네 번째 실습입니다. 어떤 변수가 중요한지 확인합니다.
>
> ```python
> importance = pd.DataFrame({
>     '특성': X.columns,
>     '중요도': model.feature_importances_
> }).sort_values('중요도', ascending=False)
>
> print(importance)
> ```
>
> 온도가 가장 중요하다고 나오면, 온도 관리에 집중해야겠죠.
>
> 이게 랜덤포레스트의 장점이에요. 모델 전체는 해석하기 어렵지만, 중요한 변수는 알 수 있습니다.

### 실습 5: n_estimators 실험 [2분]

> 다섯 번째 실습입니다. 트리 개수를 바꿔가며 성능 변화를 봅시다.
>
> ```python
> for n_trees in [10, 50, 100, 200, 500]:
>     model_temp = RandomForestClassifier(n_estimators=n_trees)
>     model_temp.fit(X_train, y_train)
>     acc = model_temp.score(X_test, y_test)
>     print(f"트리 {n_trees}개: {acc:.1%}")
> ```
>
> 10개에서 100개로 늘리면 성능이 올라가지만, 100개에서 500개로 늘려도 별 차이가 없어요.
>
> 그래서 보통 100개 정도면 충분합니다.

### 실습 6: OOB 점수 [2분]

> 여섯 번째 실습입니다. OOB 점수를 확인해봅시다.
>
> ```python
> model_oob = RandomForestClassifier(n_estimators=100, oob_score=True)
> model_oob.fit(X_train, y_train)
>
> print(f"OOB 점수: {model_oob.oob_score_:.1%}")
> ```
>
> OOB는 Out-of-Bag의 약자예요. Bootstrap 샘플링에서 제외된 데이터로 성능을 추정합니다.
>
> 별도의 검증 세트 없이도 성능을 확인할 수 있어서 편리해요.

### 실습 7: 예측 확률 [2min]

> 일곱 번째 실습입니다. 새 데이터의 예측 확률을 봅시다.
>
> ```python
> new_data = [[90, 55, 100]]
> pred = model.predict(new_data)
> proba = model.predict_proba(new_data)
>
> print(f"불량 확률: {proba[0][1]:.1%}")
> ```
>
> 랜덤포레스트는 100개 트리의 예측을 종합하기 때문에 확률이 더 신뢰할 만해요.

---

### 정리 (3분)

#### 핵심 요약 [1.5분]

> 오늘 배운 내용을 정리하겠습니다.
>
> **앙상블 학습**은 여러 모델을 합쳐서 더 좋은 성능을 내는 방법입니다.
>
> **랜덤포레스트**는 다양한 의사결정나무들의 투표입니다. 데이터와 특성을 랜덤하게 선택해서 트리마다 다르게 만들어요.
>
> **n_estimators**는 트리 개수인데, 100개 정도면 충분합니다.
>
> **특성 중요도**로 어떤 변수가 중요한지 알 수 있습니다.

#### 다음 차시 예고 [1분]

> 다음 13차시에서는 **회귀 모델**을 배웁니다.
>
> 지금까지 분류(불량/정상)를 했는데, 이제 숫자 예측(생산량, 불량률)을 합니다.
>
> 선형회귀와 다항회귀를 다룰 거예요.

#### 마무리 [0.5분]

> 앙상블의 힘을 경험했습니다.
>
> 실무에서는 랜덤포레스트를 정말 많이 씁니다. 오늘 배운 내용 잘 기억해두세요.
>
> 수고하셨습니다!

---

## 강의 노트

### 준비물
- PPT 슬라이드 (slides.md)
- 실습 코드 파일 (code.py)

### 주의사항
- 앙상블 개념 충분히 설명 (집단 지성 비유)
- 의사결정나무와 성능 비교 강조
- n_estimators 실험으로 직접 확인

### 예상 질문
1. "트리 개수를 많이 하면 과대적합 되나요?"
   → 랜덤포레스트는 트리가 많아도 과대적합이 잘 안 됨

2. "랜덤포레스트가 항상 의사결정나무보다 좋나요?"
   → 대부분 그렇지만, 해석이 필요하면 의사결정나무가 나음

3. "실무에서 가장 많이 쓰는 모델은?"
   → 랜덤포레스트, XGBoost, LightGBM 등 앙상블 모델

4. "n_estimators를 1000개 하면?"
   → 시간만 오래 걸리고 성능 향상은 미미
