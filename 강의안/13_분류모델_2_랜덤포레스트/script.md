# [13차시] 분류 모델 (2): 랜덤포레스트 - 강사 스크립트

## 강의 정보
- **차시**: 13차시 (25분)
- **유형**: 이론 + 실습
- **대상**: AI 기초체력훈련 수강생 (비전공자/입문자)

---

## 도입 (3분)

### 인사 및 지난 시간 복습 [1.5분]

> 안녕하세요, 13차시를 시작하겠습니다.
>
> 지난 시간에 의사결정트리를 배웠습니다. 해석하기 쉽다는 장점이 있었죠. 하지만 단점도 있었어요. 데이터가 조금만 바뀌어도 트리가 크게 변하고, 과대적합 위험이 있었습니다.
>
> 오늘은 이 단점을 극복하는 **랜덤포레스트**를 배웁니다!

### 학습목표 안내 [1.5분]

> 오늘 수업을 마치면 다음을 할 수 있습니다.
>
> 첫째, 앙상블 학습의 개념을 설명합니다.
> 둘째, 랜덤포레스트의 원리를 이해합니다.
> 셋째, RandomForestClassifier로 분류 모델을 구축합니다.

---

## 전개 (19분)

### 섹션 1: 앙상블 학습 이해 (5분)

#### 앙상블이란 [2분]

> **앙상블(Ensemble)**은 여러 모델을 합쳐서 더 좋은 성능을 내는 방법입니다.
>
> 비유하자면 **집단 지성**이에요. 한 명의 전문가보다 여러 전문가의 의견을 모으면 더 정확한 결정을 내릴 수 있죠.
>
> "삼인행 필유아사(三人行 必有我師)" 라는 말처럼, 여럿이 모이면 더 현명해집니다.

#### 랜덤포레스트 개념 [3분]

> **랜덤포레스트(Random Forest)**는 여러 의사결정트리를 만들어 **투표**로 결정합니다.
>
> 이름 그대로 "무작위 숲"이에요. 100개의 트리를 만들고, 각각이 "정상" 또는 "불량"을 예측한 뒤, 다수결로 최종 결정을 내립니다.
>
> 72개 트리가 "정상", 28개 트리가 "불량"이라고 하면 → 최종 결과는 "정상"!
>
> 한 트리가 틀려도 다른 트리들이 보완해주니까 전체적으로 더 정확해집니다.

---

### 섹션 2: "랜덤"의 의미 (4분)

#### 데이터 랜덤 (Bootstrap) [2분]

> "랜덤"에는 두 가지 의미가 있어요.
>
> 첫째, **데이터 랜덤 샘플링**입니다. 전체 데이터에서 랜덤하게 뽑아 각 트리를 학습시켜요. 복원 추출이라서 같은 데이터가 여러 번 뽑힐 수도 있습니다.
>
> 예를 들어:
> - 트리1은 데이터 [1,1,3,5,5,6,7,8,9,9]로 학습
> - 트리2는 데이터 [2,2,3,3,4,6,7,8,10,10]으로 학습
>
> 서로 다른 데이터로 학습하니까 서로 다른 트리가 만들어져요.

#### 특성 랜덤 (Feature Sampling) [2분]

> 둘째, **특성 랜덤 선택**입니다. 각 분할에서 일부 특성만 사용해요.
>
> 예를 들어 특성이 [온도, 습도, 속도] 3개라면:
> - 트리1의 어떤 노드는 [온도, 습도]만 고려
> - 트리2의 어떤 노드는 [습도, 속도]만 고려
>
> 이렇게 하면 다양한 트리가 만들어지고, 서로의 약점을 보완하게 됩니다.

---

### 섹션 3: sklearn 구현 (7분)

#### 기본 사용법 [3분]

> *(코드 시연)*
>
> ```python
> from sklearn.ensemble import RandomForestClassifier
>
> # 모델 생성
> model = RandomForestClassifier(
>     n_estimators=100,     # 트리 100개
>     random_state=42
> )
>
> # 학습
> model.fit(X_train, y_train)
>
> # 예측
> y_pred = model.predict(X_test)
>
> # 정확도
> accuracy = model.score(X_test, y_test)
> ```
>
> 의사결정트리와 사용법이 거의 같죠? sklearn의 일관된 API 덕분입니다.

#### 의사결정트리와 비교 [2분]

> 같은 데이터로 두 모델을 비교해볼게요.
>
> *(코드 시연)*
>
> 의사결정트리: 82%
> 랜덤포레스트: 86%
>
> 대부분의 경우 랜덤포레스트가 더 높은 정확도를 보여줍니다.

#### 특성 중요도 [2min]

> 랜덤포레스트도 특성 중요도를 제공합니다.
>
> *(코드 시연)*
>
> ```python
> importance = model.feature_importances_
> # 온도: 45%, 습도: 32%, 속도: 23%
> ```
>
> 여러 트리의 중요도를 평균낸 값이라 더 안정적이에요.

---

### 섹션 4: 하이퍼파라미터 (3분)

#### 주요 파라미터 [2분]

> 랜덤포레스트의 주요 파라미터를 살펴볼게요.
>
> ```python
> model = RandomForestClassifier(
>     n_estimators=100,     # 트리 개수
>     max_depth=10,         # 각 트리 최대 깊이
>     random_state=42
> )
> ```
>
> **n_estimators**: 트리 개수예요. 많을수록 좋지만 어느 정도 이상은 효과가 미미해요. 보통 100~200개면 충분합니다.
>
> **max_depth**: 각 트리의 깊이 제한이에요. 의사결정트리에서 배운 것처럼 과대적합 방지에 도움됩니다.

#### n_jobs 파라미터 [1분]

> 트리가 많으면 학습 시간이 오래 걸릴 수 있어요. 이때 **n_jobs=-1**을 사용하면 모든 CPU 코어를 활용해서 병렬 처리합니다.
>
> ```python
> model = RandomForestClassifier(n_estimators=200, n_jobs=-1)
> ```

---

## 정리 (3분)

### 핵심 내용 요약 [1.5분]

> 오늘 배운 핵심 내용을 정리하면:
>
> 1. **앙상블**: 여러 모델을 합쳐 더 좋은 성능
> 2. **랜덤포레스트**: 여러 의사결정트리의 투표
> 3. **랜덤**: 데이터 샘플링 + 특성 선택
> 4. **n_estimators**: 트리 개수 (100~200 권장)
> 5. **feature_importances_**: 특성 중요도
>
> 랜덤포레스트는 **실무에서 가장 많이 사용되는 모델** 중 하나입니다!

### 다음 차시 예고 [1min]

> 다음 14차시에서는 **회귀 모델**을 배웁니다.
>
> 분류(범주 예측)가 끝났으니 이제 회귀(숫자 예측)를 다룹니다. 선형회귀와 다항회귀를 실습해요.

### 마무리 인사 [0.5분]

> 앙상블의 힘을 경험하셨습니다. 수고하셨습니다!

---

## 강의 노트

### 예상 질문
1. "트리를 몇 개 만들어야 하나요?"
   → 보통 100~200개. 그 이상은 효과 미미, 시간만 증가

2. "의사결정트리는 왜 배웠나요?"
   → 랜덤포레스트의 기반. 해석이 필요할 때는 단일 트리 사용

3. "다른 앙상블 방법도 있나요?"
   → 부스팅(XGBoost, LightGBM) 등. 더 복잡하지만 성능 좋음

### 시간 조절 팁
- 시간 부족: OOB 점수 부분 생략
- 시간 여유: n_estimators에 따른 성능 변화 실험
