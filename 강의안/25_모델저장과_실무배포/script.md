# [25차시] 모델 저장과 실무 배포 준비 - 강사 스크립트

## 강의 정보
- **차시**: 25차시 (25분) - 마지막 차시
- **유형**: 실습 중심
- **대상**: AI 기초체력훈련 수강생 (비전공자/입문자)

---

## 도입 (3분)

### 인사 및 지난 시간 복습 [1.5분]

> 안녕하세요, 마지막 25차시를 시작하겠습니다.
>
> 지난 시간에 모델 해석과 특성 중요도 분석을 배웠습니다. 모델이 왜 그렇게 예측하는지 설명할 수 있게 됐죠.
>
> 오늘은 **모델 저장과 배포 준비**를 배웁니다. 학습한 모델을 파일로 저장하고, 나중에 불러와서 사용하는 방법이에요!

### 학습목표 안내 [1.5분]

> 오늘 수업을 마치면 다음을 할 수 있습니다.
>
> 첫째, joblib으로 모델을 저장하고 불러옵니다.
> 둘째, 모델 버전 관리 방법을 이해합니다.
> 셋째, 실무 배포 체크리스트를 활용합니다.
>
> 이 수업이 끝나면 전체 ML 워크플로우를 완성하게 됩니다!

---

## 전개 (19분)

### 섹션 1: 모델 저장의 필요성 (3분)

#### 왜 저장해야 하나? [1.5분]

> 모델 학습에는 시간이 걸려요. 간단한 모델도 몇 분, 복잡한 모델은 몇 시간이 걸리죠.
>
> 매번 예측할 때마다 학습하면 시간 낭비예요.
>
> 그래서 한 번 학습한 모델을 **저장**해두고, 나중에 **불러와서** 사용해요.

#### 저장 시나리오 [1.5분]

> 실제로 이렇게 사용해요:
>
> 1. 개발 환경에서 모델 학습 (Jupyter 노트북)
> 2. 모델 파일로 저장 (model.pkl)
> 3. API 서버에서 파일 로드
> 4. 요청이 오면 바로 예측!
>
> 학습은 한 번, 예측은 무한 번 할 수 있어요.

---

### 섹션 2: joblib 사용법 (7min)

#### 기본 저장/불러오기 [3min]

> *(코드 시연)*
>
> ```python
> import joblib
>
> # 저장
> joblib.dump(model, 'model.pkl')
>
> # 불러오기
> loaded_model = joblib.load('model.pkl')
> ```
>
> 이게 전부예요! 정말 간단하죠?
>
> joblib은 scikit-learn에서 공식 권장하는 방법이에요.
> NumPy 배열에 최적화되어 있어서 ML 모델에 적합해요.

#### 전처리기도 함께 [2min]

> *(코드 시연)*
>
> 주의할 점! 전처리기도 같이 저장해야 해요.
>
> ```python
> # 학습할 때
> scaler = StandardScaler()
> X_scaled = scaler.fit_transform(X_train)
> model.fit(X_scaled, y_train)
>
> # 저장
> joblib.dump(scaler, 'scaler.pkl')
> joblib.dump(model, 'model.pkl')
> ```
>
> 예측할 때 같은 스케일러를 써야 올바른 결과가 나와요!

#### 파이프라인 저장 [2min]

> *(코드 시연)*
>
> 더 깔끔한 방법이 있어요.
>
> ```python
> from sklearn.pipeline import Pipeline
>
> pipeline = Pipeline([
>     ('scaler', StandardScaler()),
>     ('model', RandomForestClassifier())
> ])
>
> pipeline.fit(X_train, y_train)
> joblib.dump(pipeline, 'pipeline.pkl')
> ```
>
> 파이프라인으로 묶으면 하나의 파일만 관리하면 돼요!

---

### 섹션 3: 모델 버전 관리 (4min)

#### 파일명 규칙 [2min]

> 실무에서는 모델이 여러 번 업데이트돼요.
>
> 파일명에 규칙을 정해두면 관리가 편해요:
>
> ```
> quality_model_v1.0_20260101.pkl
> quality_model_v2.0_20260201.pkl
> ```
>
> 버전, 날짜를 넣으면 어떤 모델인지 바로 알 수 있어요.

#### 메타데이터 저장 [2min]

> *(코드 시연)*
>
> 더 좋은 방법은 메타데이터를 함께 저장하는 거예요.
>
> ```python
> model_info = {
>     'model': model,
>     'scaler': scaler,
>     'version': '2.0',
>     'accuracy': 0.92,
>     'trained_date': '2026-01-15'
> }
> joblib.dump(model_info, 'model_package.pkl')
> ```
>
> 불러올 때 버전과 성능을 바로 확인할 수 있어요!

---

### 섹션 4: 실무 배포 준비 (5min)

#### 체크리스트 [2min]

> 배포 전에 확인해야 할 것들이에요:
>
> 1. **모델 검증**: 테스트 데이터 성능 OK?
> 2. **파일 확인**: model.pkl, scaler.pkl 있음?
> 3. **환경 확인**: Python 버전, 라이브러리 버전 일치?
>
> 특히 scikit-learn 버전이 다르면 모델 로드가 실패할 수 있어요!

#### requirements.txt [1.5min]

> *(코드 시연)*
>
> ```bash
> pip freeze > requirements.txt
> ```
>
> 현재 환경의 모든 패키지 버전을 저장해요.
>
> 다른 환경에서 이 파일로 똑같이 설치하면 됩니다:
>
> ```bash
> pip install -r requirements.txt
> ```

#### 폴더 구조 [1.5min]

> 실무 프로젝트는 이런 구조를 많이 써요:
>
> ```
> ml_project/
> ├── models/
> │   └── model_v2.0.pkl
> ├── app/
> │   └── main.py
> ├── requirements.txt
> └── README.md
> ```
>
> models 폴더에 모델, app 폴더에 API 코드를 분리해요.

---

## 정리 (3분)

### 핵심 내용 요약 [1min]

> 오늘 배운 핵심 내용:
>
> 1. **joblib.dump()**: 모델 저장
> 2. **joblib.load()**: 모델 불러오기
> 3. **전처리기도 저장**: scaler 잊지 말기
> 4. **파이프라인**: 하나로 묶어서 관리
> 5. **requirements.txt**: 환경 고정

### 전체 과정 정리 [1.5min]

> 25차시 동안 배운 내용을 정리할게요.
>
> Part I: Python 기초와 데이터 다루기
> Part II: 통계와 데이터 분석
> Part III: 머신러닝과 딥러닝
> Part IV: API, 웹앱, 배포까지
>
> 데이터 수집부터 모델 배포까지 전 과정을 경험했습니다!
>
> 이제 여러분은 AI 프로젝트의 기본 워크플로우를 이해하셨어요.

### 마무리 인사 [0.5분]

> 25차시 AI 기초체력훈련을 모두 마쳤습니다.
>
> 오늘 배운 내용을 바탕으로 실제 프로젝트에 적용해보세요.
>
> 수고 많으셨습니다. 감사합니다!

---

## 강의 노트

### 예상 질문

1. "pickle과 joblib 중 뭘 써야 하나요?"
   → ML 모델은 joblib 권장. NumPy 배열에 최적화

2. "모델 파일 용량이 너무 크면?"
   → joblib의 compress 옵션 사용 가능

3. "버전이 달라서 로드 안 되면?"
   → 같은 버전 환경에서 다시 학습 필요

### 시간 조절 팁
- 시간 부족: 폴더 구조 예시 생략
- 시간 여유: Docker 배포 간단 소개

### 실습 팁
- .pkl 파일이 Git에 올라가지 않도록 .gitignore 설정
- 모델 파일은 용량이 크므로 별도 저장소(S3 등) 권장
