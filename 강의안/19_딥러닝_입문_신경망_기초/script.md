# [19차시] 딥러닝 입문: 신경망 기초 - 강사 스크립트

## 강의 정보
- **차시**: 19차시 (25분)
- **유형**: 이론 중심
- **대상**: AI 기초체력훈련 수강생 (비전공자/입문자)

---

## 도입 (3분)

### 인사 및 지난 시간 복습 [1.5분]

> 안녕하세요, 19차시를 시작하겠습니다.
>
> 지난 시간에 시계열 예측 모델을 배웠습니다. 과거 데이터로 미래를 예측하는 특성 엔지니어링과 모델 구축을 실습했죠.
>
> 오늘부터 2차시에 걸쳐 **딥러닝**을 배웁니다! 오늘은 딥러닝의 핵심인 신경망의 원리를 이해하고, 다음 시간에 직접 구현해봅니다.

### 학습목표 안내 [1.5분]

> 오늘 수업을 마치면 다음을 할 수 있습니다.
>
> 첫째, 신경망의 기본 구조를 이해합니다.
> 둘째, 뉴런과 층의 개념을 설명합니다.
> 셋째, 딥러닝과 머신러닝의 차이를 구분합니다.

---

## 전개 (19분)

### 섹션 1: 딥러닝이란? (4분)

#### 딥러닝 개념 [2min]

> **딥러닝(Deep Learning)**은 머신러닝의 한 분야입니다. **여러 층의 신경망**으로 학습하는 방법이에요.
>
> 지금까지 배운 선형회귀, 랜덤포레스트도 머신러닝이죠. 딥러닝은 그 중에서 신경망을 사용하는 접근법입니다.
>
> "Deep"은 "깊다"는 뜻인데, 신경망의 **층이 깊다**는 의미예요.

#### 왜 딥러닝인가 [2min]

> 딥러닝은 **복잡한 패턴**을 학습하는 데 강력합니다.
>
> 전통적인 ML에서는 특성 엔지니어링을 사람이 해야 했죠. 온도, 습도에서 유용한 특성을 만들어냈어요.
>
> 딥러닝은 **스스로 특성을 학습**합니다. 이미지에서 "이것은 고양이다"라고 판단할 때, 귀 모양, 눈 위치 같은 특성을 자동으로 찾아내요.
>
> 특히 이미지, 텍스트, 음성 같은 복잡한 데이터에서 강력합니다.

---

### 섹션 2: 신경망의 구조 (8min)

#### 생물학적 뉴런 [2min]

> 인공 신경망은 **뇌의 뉴런**에서 영감을 받았습니다.
>
> 뇌의 뉴런은 수상돌기로 입력을 받고, 세포체에서 처리하고, 축삭돌기로 출력을 전달해요.
>
> 여러 곳에서 신호가 들어오면 합산해서 임계값을 넘으면 활성화되죠.

#### 인공 뉴런 [3min]

> 인공 뉴런도 비슷해요.
>
> ```
> 출력 = 활성화함수(입력₁×가중치₁ + 입력₂×가중치₂ + ... + 편향)
> ```
>
> **가중치(weight)**는 각 입력의 중요도입니다. 중요한 입력은 가중치가 크고, 덜 중요한 건 작아요. 이 값을 학습으로 찾아냅니다.
>
> **편향(bias)**은 기준점을 조정해요.
>
> **활성화 함수**는 비선형성을 추가합니다. 이게 없으면 아무리 층을 쌓아도 선형 변환일 뿐이에요.

#### 층의 구조 [3min]

> 뉴런을 쌓으면 **층(Layer)**이 됩니다.
>
> - **입력층**: 데이터가 들어오는 층
> - **은닉층**: 중간에서 특성을 추출하는 층
> - **출력층**: 최종 예측을 내보내는 층
>
> **다층 퍼셉트론(MLP)**은 이 층들이 완전히 연결된 가장 기본적인 신경망이에요.
>
> 은닉층이 많으면 **깊은** 신경망, 그래서 "딥러닝"입니다!

---

### 섹션 3: 활성화 함수와 학습 (5min)

#### 활성화 함수 [2min]

> 주요 활성화 함수를 알아볼게요.
>
> **ReLU**: 음수는 0, 양수는 그대로. 현재 가장 많이 사용돼요. 간단하고 효과적이죠.
>
> **Sigmoid**: 0~1 사이로 압축해요. 확률을 출력할 때, 분류 문제의 마지막 층에 사용합니다.
>
> 은닉층에는 보통 ReLU, 출력층에는 문제에 따라 Sigmoid(이진 분류)나 Softmax(다중 분류)를 사용해요.

#### 학습 과정 [3min]

> 신경망은 어떻게 학습할까요?
>
> 1. **순전파(Forward)**: 입력을 넣어 예측값 계산
> 2. **손실 계산**: 예측값과 실제값 비교 (얼마나 틀렸나?)
> 3. **역전파(Backward)**: 가중치를 어떻게 조정해야 하는지 계산
> 4. **가중치 업데이트**: 손실이 줄어드는 방향으로 가중치 조정
>
> 이 과정을 반복하면 가중치가 점점 좋아지고, 손실이 줄어들어요.
>
> **경사하강법(Gradient Descent)**이 핵심입니다. 경사(기울기)를 따라 내려가면서 최저점(최소 손실)을 찾아요.

---

### 섹션 4: 딥러닝 vs 머신러닝 (2min)

#### 비교 [2min]

> 딥러닝과 머신러닝을 비교해볼게요.
>
> **데이터 양**: 머신러닝은 적은 데이터에서도 작동해요. 딥러닝은 데이터가 많을수록 좋습니다.
>
> **특성 추출**: 머신러닝은 우리가 특성을 설계해요. 딥러닝은 자동으로 학습합니다.
>
> **해석**: 의사결정트리는 왜 그렇게 예측했는지 알 수 있어요. 딥러닝은 블랙박스에 가깝습니다.
>
> **적합한 문제**: 제조 데이터 같은 테이블 데이터는 랜덤포레스트가 효과적일 때가 많아요. 이미지, 텍스트는 딥러닝이 강력합니다.

---

## 정리 (3분)

### 핵심 내용 요약 [1.5min]

> 오늘 배운 핵심 내용을 정리하면:
>
> 1. **딥러닝**: 여러 층의 신경망으로 학습
> 2. **뉴런**: 입력×가중치 + 편향 → 활성화 함수
> 3. **층**: 입력층, 은닉층, 출력층
> 4. **활성화 함수**: ReLU(은닉층), Sigmoid(분류 출력)
> 5. **학습**: 순전파 → 손실 → 역전파 → 가중치 업데이트
>
> 딥러닝은 복잡한 패턴을 학습하는 데 강력하지만, 데이터가 많이 필요합니다!

### 다음 차시 예고 [1min]

> 다음 20차시에서는 **Keras로 MLP를 구현**합니다.
>
> 오늘 배운 개념을 코드로 만들어보고, 제조 데이터로 품질 예측 모델을 학습해봅니다.

### 마무리 인사 [0.5분]

> 신경망의 원리를 이해했습니다. 수고하셨습니다!

---

## 강의 노트

### 예상 질문
1. "딥러닝이 항상 더 좋은가요?"
   → 아님. 테이블 데이터는 랜덤포레스트가 효과적일 때 많음

2. "층을 많이 쌓을수록 좋은가요?"
   → 어느 정도까지. 너무 깊으면 학습 어려움

3. "학습률은 얼마로 해야 하나요?"
   → 기본값(0.001)으로 시작, 조절 필요

### 시간 조절 팁
- 시간 부족: 경사하강법 수식 생략
- 시간 여유: CNN, RNN 간단 소개
