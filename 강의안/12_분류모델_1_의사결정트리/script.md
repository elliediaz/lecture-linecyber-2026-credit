# [12차시] 분류 모델 (1): 의사결정트리 - 강사 스크립트

## 강의 정보
- **차시**: 12차시 (25분)
- **유형**: 이론 + 실습
- **대상**: AI 기초체력훈련 수강생 (비전공자/입문자)

---

## 도입 (3분)

### 인사 및 지난 시간 복습 [1.5분]

> 안녕하세요, 12차시를 시작하겠습니다.
>
> 지난 시간에 머신러닝의 기본 개념을 배웠죠. 분류와 회귀, sklearn의 fit-predict 패턴을 익혔습니다.
>
> 오늘은 드디어 **첫 번째 AI 모델**을 직접 만들어봅니다!

### 학습목표 안내 [1.5분]

> 오늘 수업을 마치면 다음을 할 수 있습니다.
>
> 첫째, 의사결정트리의 원리를 설명합니다.
> 둘째, DecisionTreeClassifier로 분류 모델을 구축합니다.
> 셋째, 모델의 예측과 성능을 확인합니다.

---

## 전개 (19분)

### 섹션 1: 의사결정트리 이해 (6분)

#### 의사결정트리란 [2분]

> **의사결정트리(Decision Tree)**는 질문을 통해 단계적으로 분류하는 알고리즘입니다.
>
> 마치 스무고개 게임과 비슷해요. "온도가 88도보다 높나요?" → "예" → "습도가 60%보다 높나요?" 이런 식으로 질문해나가며 최종 결론에 도달합니다.
>
> 우리도 일상에서 이런 결정을 해요. "비 올 확률 50% 이상인가?" → "예" → "외출 2시간 넘는가?" → "예" → "우산 챙기기"

#### 트리의 구성 요소 [2분]

> 트리 구조를 살펴볼게요.
>
> - **루트 노드(Root)**: 맨 위에 있는 첫 번째 질문
> - **브랜치(Branch)**: 가지, 질문의 결과에 따라 갈라지는 경로
> - **리프 노드(Leaf)**: 맨 끝에 있는 최종 결과 (정상/불량)
>
> 뿌리에서 시작해서 가지를 타고 내려가 잎에서 결론을 얻는 구조입니다.

#### 질문은 어떻게 만들어지나 [2분]

> "어떤 질문이 좋은 질문일까요?"
>
> 데이터를 **가장 잘 나누는** 질문이 좋은 질문입니다. 이것을 **정보 이득(Information Gain)**이라고 해요.
>
> 예를 들어, "온도 > 88도"로 나눴을 때 한쪽은 불량만, 한쪽은 정상만 있다면 완벽한 질문이죠.
>
> 반면 "습도 > 50"으로 나눴는데 양쪽 다 불량과 정상이 섞여 있다면 별로 좋지 않은 질문입니다.
>
> 알고리즘이 자동으로 최적의 질문을 찾아줍니다!

---

### 섹션 2: sklearn 구현 (8분)

#### 모델 생성과 학습 [3분]

> *(코드 시연)*
>
> ```python
> from sklearn.tree import DecisionTreeClassifier
> from sklearn.model_selection import train_test_split
>
> # 데이터 준비
> X = df[['온도', '습도', '속도']]
> y = df['불량여부']
>
> # 학습/테스트 분리
> X_train, X_test, y_train, y_test = train_test_split(
>     X, y, test_size=0.2, random_state=42
> )
>
> # 모델 생성 및 학습
> model = DecisionTreeClassifier(random_state=42)
> model.fit(X_train, y_train)
> ```
>
> fit 한 줄로 학습이 끝납니다! 내부적으로 최적의 질문들을 찾아 트리를 만들어요.

#### 예측하기 [2분]

> *(코드 시연)*
>
> ```python
> # 새 데이터 예측
> new_data = [[90, 55, 100]]
> prediction = model.predict(new_data)
> print(prediction)  # [1] → 불량
> ```
>
> predict 한 줄로 예측합니다. 1은 불량, 0은 정상이에요.
>
> **predict_proba**를 사용하면 확률도 볼 수 있어요.
>
> ```python
> proba = model.predict_proba([[90, 55, 100]])
> # [[0.25, 0.75]] → 정상 25%, 불량 75%
> ```

#### 성능 평가 [3분]

> 모델이 얼마나 잘 맞추는지 확인해볼게요.
>
> *(코드 시연)*
>
> ```python
> # 정확도 확인
> accuracy = model.score(X_test, y_test)
> print(f"정확도: {accuracy:.1%}")
> ```
>
> score 메서드가 테스트 데이터로 정확도를 계산해줍니다.
>
> 정확도는 "맞은 개수 / 전체 개수"예요. 85%면 100개 중 85개를 맞췄다는 뜻이죠.
>
> 주의할 점! 학습 데이터와 테스트 데이터의 정확도를 비교하세요. 학습은 95%인데 테스트가 70%면 **과대적합**입니다.

---

### 섹션 3: 트리 시각화와 해석 (5분)

#### 트리 시각화 [2분]

> *(코드 시연)*
>
> ```python
> from sklearn.tree import plot_tree
> import matplotlib.pyplot as plt
>
> plt.figure(figsize=(15, 10))
> plot_tree(model,
>           feature_names=['온도', '습도', '속도'],
>           class_names=['정상', '불량'],
>           filled=True)
> plt.show()
> ```
>
> 트리가 그림으로 나타납니다. 의사결정트리의 가장 큰 장점이 바로 이 **해석 가능성**이에요.

#### 결과 해석 [2분]

> 그림을 보면 "온도 <= 87.5" 같은 조건이 보여요.
>
> 각 노드에서:
> - **samples**: 해당 노드에 온 데이터 수
> - **value**: [정상 수, 불량 수]
> - **class**: 예측 결과
>
> 예를 들어 "온도가 87.5도 이하면 정상 가능성 높음"이라는 규칙을 눈으로 확인할 수 있죠.
>
> 이렇게 **왜 그런 예측을 했는지 설명 가능**한 것이 의사결정트리의 장점입니다.

#### 과대적합 방지 [1분]

> 트리가 너무 깊어지면 **과대적합**이 발생해요. 학습 데이터를 외워버리는 거죠.
>
> max_depth 파라미터로 깊이를 제한하세요.
>
> ```python
> model = DecisionTreeClassifier(max_depth=5, random_state=42)
> ```

---

## 정리 (3분)

### 핵심 내용 요약 [1.5분]

> 오늘 배운 핵심 내용을 정리하면:
>
> 1. **의사결정트리**: 질문으로 단계적 분류
> 2. **DecisionTreeClassifier**: sklearn의 분류 트리
> 3. **fit → predict**: 학습하고 예측
> 4. **score**: 정확도 확인
> 5. **plot_tree**: 모델 시각화
> 6. **max_depth**: 과대적합 방지
>
> 의사결정트리는 해석이 쉬워서 실무에서 모델 설명이 필요할 때 유용합니다.

### 다음 차시 예고 [1min]

> 다음 13차시에서는 **랜덤포레스트**를 배웁니다.
>
> 의사결정트리 하나보다 **여러 개의 트리**를 합치면 성능이 더 좋아져요. 숲(Forest)을 만드는 거죠!

### 마무리 인사 [0.5분]

> 첫 AI 모델 완성을 축하드립니다! 수고하셨습니다.

---

## 강의 노트

### 예상 질문
1. "max_depth를 얼마로 설정해야 하나요?"
   → 데이터에 따라 다름. 보통 3~10 사이에서 시작해서 교차검증으로 튜닝

2. "의사결정트리만 써도 되나요?"
   → 가능하지만 성능 한계가 있음. 다음 시간에 배울 랜덤포레스트 추천

3. "random_state는 왜 넣나요?"
   → 재현성을 위해. 같은 값이면 항상 같은 결과가 나옴

### 시간 조절 팁
- 시간 부족: 과대적합 부분 간략히
- 시간 여유: feature_importances_ 소개
