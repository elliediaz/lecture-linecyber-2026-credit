# [18차시] 딥러닝 입문: 신경망 기초 - 강사 스크립트

## 강의 정보
- **차시**: 18차시 (25-30분)
- **유형**: 이론 + 실습
- **구성**: 이론 10분 + 실습 15-20분
- **대상**: 비전공자, AI 입문자, 제조업 종사자

---

## 이론편 (10분)

### 도입 (2분)

#### 인사 및 지난 시간 복습 [1분]

> 안녕하세요, 18차시를 시작하겠습니다.
>
> 지난 시간에 시계열 예측 모델을 배웠습니다. 특성 엔지니어링으로 과거 데이터를 활용해 미래를 예측했죠.
>
> 오늘은 드디어 **딥러닝**을 배웁니다. 최근 AI 기술의 핵심이 되는 신경망을 이해해보겠습니다.

#### 학습목표 안내 [1분]

> 오늘 수업을 마치면 다음 세 가지를 할 수 있습니다.
>
> 첫째, 신경망의 기본 구조를 이해합니다.
> 둘째, 뉴런과 층의 개념을 설명합니다.
> 셋째, 딥러닝과 머신러닝의 차이를 구분합니다.

---

### 핵심 내용 (8분)

#### 딥러닝이란? [1.5분]

> **딥러닝**은 여러 층의 신경망으로 학습하는 머신러닝의 한 분야입니다.
>
> 우리가 배운 선형회귀, 의사결정트리, 랜덤포레스트 같은 건 전통적인 머신러닝이고요. 딥러닝은 그 중에서 신경망을 사용하는 방법이에요.
>
> 딥러닝이 특히 강한 분야는 이미지, 텍스트, 음성입니다. 복잡한 패턴을 자동으로 학습하거든요.

#### 인공 뉴런 [2min]

> 딥러닝의 기본 단위는 **뉴런**입니다. 사람 뇌의 신경세포를 모방한 거예요.
>
> 뉴런은 간단한 계산을 합니다. 입력에 가중치를 곱하고, 편향을 더한 다음, 활성화 함수를 통과시켜요.
>
> ```python
> y = f(w1*x1 + w2*x2 + w3*x3 + b)
> ```
>
> **가중치**는 각 입력의 중요도예요. 온도가 품질에 더 영향을 미치면 온도의 가중치가 높겠죠.
>
> **편향**은 기준점을 조정하는 역할입니다.
>
> 이 가중치와 편향이 학습을 통해 자동으로 정해집니다.

#### 활성화 함수 [1.5min]

> **활성화 함수**가 왜 필요할까요?
>
> 활성화 함수가 없으면 아무리 층을 쌓아도 결국 하나의 선형 변환이에요. 복잡한 패턴을 못 배워요.
>
> 비선형 함수를 추가해야 곡선도 배울 수 있습니다.
>
> 가장 많이 쓰는 건 **ReLU**예요. 음수면 0, 양수면 그대로 통과. 아주 단순하지만 효과적입니다.
>
> ```python
> relu(x) = max(0, x)
> ```

#### 신경망 구조 [1.5min]

> 뉴런 여러 개를 모으면 **층(Layer)**이 됩니다.
>
> 신경망은 세 종류의 층으로 구성돼요.
> - **입력층**: 데이터를 받는 층
> - **은닉층**: 특성을 학습하는 층
> - **출력층**: 결과를 내는 층
>
> 은닉층이 2개 이상이면 **심층 신경망(Deep Neural Network)**이라고 해요. 그래서 "딥"러닝입니다.

#### 학습 과정 [1.5min]

> 신경망은 어떻게 학습할까요?
>
> **순전파**에서는 입력이 들어가서 예측값이 나옵니다.
>
> 예측값과 실제값의 차이를 **손실**이라고 해요.
>
> **역전파**에서는 이 손실을 줄이도록 가중치를 조정합니다. 경사하강법으로 조금씩 최적의 가중치를 찾아가요.
>
> 이 과정을 여러 번 반복하면 점점 예측이 좋아집니다.

---

## 실습편 (15-20분)

### 실습 소개 [2분]

> 이제 실습 시간입니다. NumPy로 신경망의 기본 원리를 직접 구현해봅니다.
>
> **실습 목표**입니다.
> 1. 뉴런 계산을 구현합니다.
> 2. 활성화 함수를 이해합니다.
> 3. 간단한 신경망 구조를 만듭니다.
>
> **실습 환경**을 확인해주세요.
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> ```

### 실습 1: 뉴런 계산 [2min]

> 첫 번째 실습입니다. 뉴런 하나의 계산을 구현합니다.
>
> ```python
> inputs = np.array([85, 50, 100])  # 온도, 습도, 속도
> weights = np.array([0.3, 0.2, 0.5])
> bias = 0.1
>
> z = np.dot(inputs, weights) + bias
> ```
>
> np.dot으로 입력과 가중치를 곱해서 더하고, 편향을 더합니다.

### 실습 2: 활성화 함수 [2min]

> 두 번째 실습입니다. ReLU와 Sigmoid를 구현합니다.
>
> ```python
> def relu(x):
>     return np.maximum(0, x)
>
> def sigmoid(x):
>     return 1 / (1 + np.exp(-x))
> ```
>
> ReLU는 음수를 0으로 만들고, Sigmoid는 0과 1 사이로 변환합니다.

### 실습 3: 시각화 [2min]

> 세 번째 실습입니다. 활성화 함수를 그래프로 그려봅니다.
>
> ReLU는 0에서 꺾이는 직선이고, Sigmoid는 S자 곡선이에요.
>
> 이 그래프가 왜 비선형인지 느껴보세요.

### 실습 4: 단일 뉴런 클래스 [3min]

> 네 번째 실습입니다. 뉴런을 클래스로 만듭니다.
>
> ```python
> class Neuron:
>     def __init__(self, n_inputs):
>         self.weights = np.random.randn(n_inputs) * 0.1
>         self.bias = 0
>
>     def forward(self, inputs):
>         z = np.dot(inputs, self.weights) + self.bias
>         return relu(z)
> ```
>
> 가중치는 처음에 랜덤한 작은 값으로 초기화해요.

### 실습 5: 층 구현 [3min]

> 다섯 번째 실습입니다. 여러 뉴런을 모은 층을 만듭니다.
>
> ```python
> class Layer:
>     def __init__(self, n_inputs, n_neurons):
>         self.weights = np.random.randn(n_inputs, n_neurons) * 0.1
>         self.biases = np.zeros(n_neurons)
>
>     def forward(self, inputs):
>         z = np.dot(inputs, self.weights) + self.biases
>         return relu(z)
> ```
>
> 3개 입력, 4개 뉴런이면 가중치 행렬은 3×4가 됩니다.

### 실습 6: 2층 신경망 [3min]

> 여섯 번째 실습입니다. 층을 두 개 연결합니다.
>
> ```python
> layer1 = Layer(3, 4)   # 입력 3 → 뉴런 4
> layer2 = Layer(4, 1)   # 뉴런 4 → 출력 1
>
> hidden = layer1.forward(inputs)
> output = layer2.forward(hidden)
> ```
>
> 첫 번째 층의 출력이 두 번째 층의 입력이 됩니다. 이게 순전파예요.

### 실습 7: 손실 계산 [2min]

> 마지막 실습입니다. 손실을 계산합니다.
>
> ```python
> def mse_loss(y_true, y_pred):
>     return np.mean((y_true - y_pred) ** 2)
> ```
>
> 손실이 작아지도록 가중치를 조정하는 게 학습입니다. 다음 시간에 실제로 해볼 거예요.

---

### 정리 (3분)

#### 핵심 요약 [1.5min]

> 오늘 배운 내용을 정리하겠습니다.
>
> **뉴런**은 입력 × 가중치 + 편향 → 활성화 함수를 거쳐 출력을 만듭니다.
>
> **활성화 함수**는 비선형성을 추가해서 복잡한 패턴을 학습할 수 있게 해요. ReLU가 가장 많이 쓰입니다.
>
> **층**은 뉴런들의 집합이고, 은닉층이 2개 이상이면 딥러닝입니다.
>
> **역전파**로 손실을 줄이는 방향으로 가중치를 조정합니다.

#### 다음 차시 예고 [1min]

> 다음 19차시에서는 **MLP로 품질 예측**을 합니다.
>
> 오늘은 NumPy로 원리를 배웠는데, 다음 시간에는 Keras라는 딥러닝 프레임워크를 써서 실제 제조 품질 예측을 해봅니다.

#### 마무리 [0.5min]

> 인공지능의 핵심, 신경망을 이해했습니다. 수고하셨습니다!

---

## 강의 노트

### 준비물
- PPT 슬라이드 (slides.md)
- 실습 코드 파일 (code.py)

### 주의사항
- 수학 공식보다 직관적 이해에 집중
- 생물학적 뉴런과의 비유 활용
- 딥러닝 vs ML 차이점 명확히

### 예상 질문
1. "딥러닝이 항상 더 좋은가요?"
   → 아님. 테이블 데이터는 RandomForest가 더 좋을 수 있음. 데이터와 문제에 따라 다름

2. "은닉층을 많이 쌓으면 좋은가요?"
   → 무조건은 아님. 과대적합, 학습 어려움 발생. 적절한 깊이가 중요

3. "활성화 함수를 왜 ReLU를 많이 쓰나요?"
   → 계산 빠르고, 기울기 소실 문제 적음. Sigmoid는 깊은 신경망에서 문제 발생

4. "가중치를 어떻게 정하나요?"
   → 처음엔 랜덤, 학습하면서 역전파로 조정됨
