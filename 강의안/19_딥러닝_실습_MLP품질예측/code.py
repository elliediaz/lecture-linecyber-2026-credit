# [19ì°¨ì‹œ] ë”¥ëŸ¬ë‹ ì‹¤ìŠµ: MLPë¡œ í’ˆì§ˆ ì˜ˆì¸¡ - ì‹¤ìŠµ ì½”ë“œ

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from sklearn.ensemble import RandomForestClassifier

# TensorFlow/Keras
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'Malgun Gothic'
plt.rcParams['axes.unicode_minus'] = False

print("=" * 60)
print("19ì°¨ì‹œ: ë”¥ëŸ¬ë‹ ì‹¤ìŠµ - MLPë¡œ í’ˆì§ˆ ì˜ˆì¸¡")
print("Kerasë¡œ ì²« ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ë§Œë“¤ì–´ë´…ë‹ˆë‹¤!")
print("=" * 60)
print(f"\nTensorFlow ë²„ì „: {tf.__version__}")
print()


# ============================================================
# ì‹¤ìŠµ 1: ì œì¡° ë°ì´í„° ìƒì„±
# ============================================================
print("=" * 50)
print("ì‹¤ìŠµ 1: ì œì¡° ë°ì´í„° ìƒì„±")
print("=" * 50)

np.random.seed(42)
n_samples = 1000

# ì…ë ¥ íŠ¹ì„±: ì˜¨ë„, ìŠµë„, ì†ë„
temperature = np.random.normal(85, 5, n_samples)
humidity = np.random.normal(50, 10, n_samples)
speed = np.random.normal(100, 15, n_samples)

# ë¶ˆëŸ‰ í™•ë¥  (ì˜¨ë„, ìŠµë„ ì˜í–¥)
defect_prob = 0.05 + 0.03 * (temperature - 80) / 5 + 0.02 * (humidity - 40) / 10
defect = (np.random.random(n_samples) < defect_prob).astype(int)

# ë°ì´í„°ì…‹ êµ¬ì„±
X = np.column_stack([temperature, humidity, speed])
y = defect

print(f"ë°ì´í„° í¬ê¸°: {X.shape}")
print(f"ë¶ˆëŸ‰ ë¹„ìœ¨: {y.mean():.1%}")
print(f"ë¶ˆëŸ‰: {y.sum()}ê°œ, ì •ìƒ: {len(y) - y.sum()}ê°œ")
print()


# ============================================================
# ì‹¤ìŠµ 2: ë°ì´í„° ë¶„í•  ë° ì •ê·œí™”
# ============================================================
print("=" * 50)
print("ì‹¤ìŠµ 2: ë°ì´í„° ë¶„í•  ë° ì •ê·œí™”")
print("=" * 50)

# Train/Test ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"Train: {len(X_train)}ê°œ, Test: {len(X_test)}ê°œ")

# ì •ê·œí™” (ë”¥ëŸ¬ë‹ì—ì„œ ì¤‘ìš”!)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(f"\nì •ê·œí™” ì „ í‰ê· : {X_train.mean(axis=0)}")
print(f"ì •ê·œí™” í›„ í‰ê· : {X_train_scaled.mean(axis=0).round(2)}")
print(f"ì •ê·œí™” í›„ í‘œì¤€í¸ì°¨: {X_train_scaled.std(axis=0).round(2)}")
print()


# ============================================================
# ì‹¤ìŠµ 3: MLP ëª¨ë¸ êµ¬ì¶•
# ============================================================
print("=" * 50)
print("ì‹¤ìŠµ 3: MLP ëª¨ë¸ êµ¬ì¶•")
print("=" * 50)

# Sequential ëª¨ë¸ ìƒì„±
model = Sequential([
    Dense(16, activation='relu', input_shape=(3,)),  # ì…ë ¥ì¸µ â†’ ì€ë‹‰ì¸µ1
    Dense(8, activation='relu'),                      # ì€ë‹‰ì¸µ1 â†’ ì€ë‹‰ì¸µ2
    Dense(1, activation='sigmoid')                    # ì€ë‹‰ì¸µ2 â†’ ì¶œë ¥ì¸µ
])

print("ëª¨ë¸ êµ¬ì¡°:")
print("-" * 50)
model.summary()
print()


# ============================================================
# ì‹¤ìŠµ 4: ëª¨ë¸ ì»´íŒŒì¼
# ============================================================
print("=" * 50)
print("ì‹¤ìŠµ 4: ëª¨ë¸ ì»´íŒŒì¼")
print("=" * 50)

model.compile(
    optimizer='adam',           # ìµœì í™” ì•Œê³ ë¦¬ì¦˜
    loss='binary_crossentropy', # ì´ì§„ ë¶„ë¥˜ ì†ì‹¤ í•¨ìˆ˜
    metrics=['accuracy']        # í‰ê°€ ì§€í‘œ
)

print("ì»´íŒŒì¼ ì„¤ì •:")
print(f"  - optimizer: adam")
print(f"  - loss: binary_crossentropy")
print(f"  - metrics: accuracy")
print()


# ============================================================
# ì‹¤ìŠµ 5: ëª¨ë¸ í•™ìŠµ
# ============================================================
print("=" * 50)
print("ì‹¤ìŠµ 5: ëª¨ë¸ í•™ìŠµ")
print("=" * 50)

history = model.fit(
    X_train_scaled, y_train,
    epochs=50,
    batch_size=32,
    validation_split=0.2,
    verbose=1
)

print("\ní•™ìŠµ ì™„ë£Œ!")
print(f"ìµœì¢… Train Loss: {history.history['loss'][-1]:.4f}")
print(f"ìµœì¢… Train Accuracy: {history.history['accuracy'][-1]:.4f}")
print(f"ìµœì¢… Val Loss: {history.history['val_loss'][-1]:.4f}")
print(f"ìµœì¢… Val Accuracy: {history.history['val_accuracy'][-1]:.4f}")
print()


# ============================================================
# ì‹¤ìŠµ 6: í•™ìŠµ ê³¡ì„  ì‹œê°í™”
# ============================================================
print("=" * 50)
print("ì‹¤ìŠµ 6: í•™ìŠµ ê³¡ì„  ì‹œê°í™”")
print("=" * 50)

fig, axes = plt.subplots(1, 2, figsize=(12, 4))

# Loss ê³¡ì„ 
axes[0].plot(history.history['loss'], label='Train', linewidth=2)
axes[0].plot(history.history['val_loss'], label='Validation', linewidth=2)
axes[0].set_title('Loss ê³¡ì„ ')
axes[0].set_xlabel('Epoch')
axes[0].set_ylabel('Loss')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# Accuracy ê³¡ì„ 
axes[1].plot(history.history['accuracy'], label='Train', linewidth=2)
axes[1].plot(history.history['val_accuracy'], label='Validation', linewidth=2)
axes[1].set_title('Accuracy ê³¡ì„ ')
axes[1].set_xlabel('Epoch')
axes[1].set_ylabel('Accuracy')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('í•™ìŠµ_ê³¡ì„ .png', dpi=100)
plt.close()
print("í•™ìŠµ ê³¡ì„  ì €ì¥: í•™ìŠµ_ê³¡ì„ .png")

# ê³¼ëŒ€ì í•© ì—¬ë¶€ í™•ì¸
final_train_loss = history.history['loss'][-1]
final_val_loss = history.history['val_loss'][-1]
if final_val_loss > final_train_loss * 1.2:
    print("âš ï¸ ê³¼ëŒ€ì í•© ì˜ì‹¬: Val Loss > Train Loss * 1.2")
else:
    print("âœ… ê³¼ëŒ€ì í•© ì—†ìŒ: ì •ìƒ í•™ìŠµ")
print()


# ============================================================
# ì‹¤ìŠµ 7: ì˜ˆì¸¡ ë° í‰ê°€
# ============================================================
print("=" * 50)
print("ì‹¤ìŠµ 7: ì˜ˆì¸¡ ë° í‰ê°€")
print("=" * 50)

# ì˜ˆì¸¡ (í™•ë¥ )
y_prob = model.predict(X_test_scaled, verbose=0)

# ì´ì§„ ë¶„ë¥˜ (0.5 ê¸°ì¤€)
y_pred = (y_prob > 0.5).astype(int).flatten()

# ì •í™•ë„
mlp_accuracy = accuracy_score(y_test, y_pred)
print(f"MLP ì •í™•ë„: {mlp_accuracy:.3f}")

# ìƒì„¸ ë¦¬í¬íŠ¸
print("\në¶„ë¥˜ ë¦¬í¬íŠ¸:")
print(classification_report(y_test, y_pred, target_names=['ì •ìƒ', 'ë¶ˆëŸ‰']))


# ============================================================
# ì‹¤ìŠµ 8: í˜¼ë™ í–‰ë ¬
# ============================================================
print("=" * 50)
print("ì‹¤ìŠµ 8: í˜¼ë™ í–‰ë ¬")
print("=" * 50)

cm = confusion_matrix(y_test, y_pred)
print("í˜¼ë™ í–‰ë ¬:")
print(f"          ì˜ˆì¸¡")
print(f"          ì •ìƒ  ë¶ˆëŸ‰")
print(f"ì‹¤ì œ ì •ìƒ  {cm[0,0]:4d}  {cm[0,1]:4d}")
print(f"     ë¶ˆëŸ‰  {cm[1,0]:4d}  {cm[1,1]:4d}")
print()


# ============================================================
# ì‹¤ìŠµ 9: RandomForestì™€ ë¹„êµ
# ============================================================
print("=" * 50)
print("ì‹¤ìŠµ 9: RandomForestì™€ ë¹„êµ")
print("=" * 50)

# RandomForest í•™ìŠµ (ì •ê·œí™” ë¶ˆí•„ìš”)
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
rf_pred = rf.predict(X_test)
rf_accuracy = accuracy_score(y_test, rf_pred)

print("ì„±ëŠ¥ ë¹„êµ:")
print("-" * 40)
print(f"{'ëª¨ë¸':<20} {'ì •í™•ë„':<15}")
print("-" * 40)
print(f"{'MLP (Keras)':<20} {mlp_accuracy:<15.3f}")
print(f"{'RandomForest':<20} {rf_accuracy:<15.3f}")
print("-" * 40)

if mlp_accuracy > rf_accuracy:
    print("\nâ˜… MLPê°€ ë” ì¢‹ìŒ")
elif rf_accuracy > mlp_accuracy:
    print("\nâ˜… RandomForestê°€ ë” ì¢‹ìŒ")
else:
    print("\nâ˜… ë‘ ëª¨ë¸ ì„±ëŠ¥ ë™ì¼")

print("\nğŸ’¡ í…Œì´ë¸” ë°ì´í„°ì—ì„œëŠ” MLê³¼ DL ì„±ëŠ¥ì´ ë¹„ìŠ·í•˜ê±°ë‚˜ MLì´ ë‚˜ì„ ìˆ˜ ìˆìŒ")
print()


# ============================================================
# ì‹¤ìŠµ 10: ëª¨ë¸ ì €ì¥
# ============================================================
print("=" * 50)
print("ì‹¤ìŠµ 10: ëª¨ë¸ ì €ì¥")
print("=" * 50)

# ëª¨ë¸ ì €ì¥
model.save('mlp_defect_model.keras')
print("ëª¨ë¸ ì €ì¥: mlp_defect_model.keras")

# ëª¨ë¸ ë¡œë“œ (ì˜ˆì‹œ)
# loaded_model = keras.models.load_model('mlp_defect_model.keras')
print()


# ============================================================
# ì‹¤ìŠµ 11: ìƒˆ ë°ì´í„° ì˜ˆì¸¡
# ============================================================
print("=" * 50)
print("ì‹¤ìŠµ 11: ìƒˆ ë°ì´í„° ì˜ˆì¸¡")
print("=" * 50)

# ìƒˆë¡œìš´ ì œì¡° ì¡°ê±´
new_data = np.array([
    [90, 60, 105],  # ë†’ì€ ì˜¨ë„, ë†’ì€ ìŠµë„
    [80, 45, 100],  # ì •ìƒ ì¡°ê±´
    [95, 55, 110]   # ë§¤ìš° ë†’ì€ ì˜¨ë„
])

# ì •ê·œí™”
new_data_scaled = scaler.transform(new_data)

# ì˜ˆì¸¡
new_probs = model.predict(new_data_scaled, verbose=0)

print("ìƒˆ ë°ì´í„° ì˜ˆì¸¡:")
print("-" * 50)
for i, (data, prob) in enumerate(zip(new_data, new_probs)):
    label = "ë¶ˆëŸ‰" if prob > 0.5 else "ì •ìƒ"
    print(f"ì¡°ê±´ {i+1}: ì˜¨ë„={data[0]:.0f}, ìŠµë„={data[1]:.0f}, ì†ë„={data[2]:.0f}")
    print(f"         ë¶ˆëŸ‰ í™•ë¥ : {prob[0]:.1%} â†’ ì˜ˆì¸¡: {label}")
print()


# ============================================================
# í•µì‹¬ ìš”ì•½
# ============================================================
print("=" * 50)
print("í•µì‹¬ ìš”ì•½")
print("=" * 50)

print(f"""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Keras MLP í•µì‹¬ ì •ë¦¬                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                        â”‚
â”‚  â–¶ ëª¨ë¸ êµ¬ì¶•                                           â”‚
â”‚     model = Sequential([                               â”‚
â”‚         Dense(16, activation='relu', input_shape=(3,)),â”‚
â”‚         Dense(8, activation='relu'),                   â”‚
â”‚         Dense(1, activation='sigmoid')                 â”‚
â”‚     ])                                                 â”‚
â”‚                                                        â”‚
â”‚  â–¶ ì»´íŒŒì¼                                              â”‚
â”‚     model.compile(                                     â”‚
â”‚         optimizer='adam',                              â”‚
â”‚         loss='binary_crossentropy',                    â”‚
â”‚         metrics=['accuracy']                           â”‚
â”‚     )                                                  â”‚
â”‚                                                        â”‚
â”‚  â–¶ í•™ìŠµ                                                â”‚
â”‚     history = model.fit(                               â”‚
â”‚         X_train, y_train,                              â”‚
â”‚         epochs=50, batch_size=32,                      â”‚
â”‚         validation_split=0.2                           â”‚
â”‚     )                                                  â”‚
â”‚                                                        â”‚
â”‚  â–¶ ì˜ˆì¸¡                                                â”‚
â”‚     y_prob = model.predict(X_test)                     â”‚
â”‚     y_pred = (y_prob > 0.5).astype(int)                â”‚
â”‚                                                        â”‚
â”‚  â˜… ë°ì´í„° ì •ê·œí™” í•„ìˆ˜! (StandardScaler)                â”‚
â”‚  â˜… í•™ìŠµ ê³¡ì„ ìœ¼ë¡œ ê³¼ëŒ€ì í•© ê°ì§€!                         â”‚
â”‚  â˜… í…Œì´ë¸” ë°ì´í„°ëŠ” MLê³¼ ë¹„êµ í•„ìš”!                      â”‚
â”‚                                                        â”‚
â”‚  ê²°ê³¼:                                                 â”‚
â”‚    MLP ì •í™•ë„: {mlp_accuracy:.3f}                                â”‚
â”‚    RandomForest ì •í™•ë„: {rf_accuracy:.3f}                        â”‚
â”‚                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ë‹¤ìŒ ì°¨ì‹œ: AI APIì˜ ì´í•´ì™€ í™œìš©
""")

print("=" * 60)
print("19ì°¨ì‹œ ì‹¤ìŠµ ì™„ë£Œ!")
print("Kerasë¡œ ì²« ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤!")
print("=" * 60)
