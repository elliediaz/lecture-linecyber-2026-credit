# [14차시] 분류 모델 (2) - 랜덤포레스트 - 강사 스크립트

## 📋 수업 개요

| 항목 | 내용 |
|------|------|
| 차시 | 14차시 |
| 주제 | 분류 모델 - 랜덤포레스트 |
| 시간 | 30분 (이론 15분 + 실습 13분 + 정리 2분) |
| 학습 목표 | 앙상블 학습 이해, 랜덤포레스트 원리, 실습 |

---

## 🎯 학습 목표

1. 앙상블 학습의 개념을 설명한다
2. 랜덤포레스트의 원리를 이해한다
3. RandomForestClassifier를 사용한다

---

## 🕐 시간 배분

| 구간 | 시간 | 내용 |
|------|------|------|
| 도입 | 2분 | 복습 및 학습목표 |
| Part 1 | 5분 | 앙상블 학습 개념 |
| Part 2 | 5분 | 랜덤포레스트 원리 |
| Part 3 | 5분 | sklearn 실습 |
| 실습 | 11분 | 의사결정나무와 비교 |
| 정리 | 2분 | 요약 및 다음 차시 예고 |

---

## 📝 상세 스크립트

### 도입부 (2분)

#### 슬라이드 1-2: 복습

> "지난 시간에 의사결정나무를 배웠습니다. 직관적이고 해석하기 쉬운 모델이었죠. 하지만 단점도 있었습니다. 기억나시나요?"

> "네, 과대적합되기 쉽고, 데이터가 조금만 바뀌어도 트리 구조가 크게 달라지는 불안정함이 있었습니다."

> "오늘은 이 문제를 해결한 '랜덤포레스트'를 배웁니다. 나무 하나가 불안정하면, 나무 여러 개를 모아서 숲을 만들면 됩니다!"

---

#### 슬라이드 3-4: 학습 목표

> "오늘의 학습 목표는 세 가지입니다."

> "첫째, 앙상블 학습이 뭔지 개념을 이해합니다. 둘째, 랜덤포레스트가 어떻게 작동하는지 원리를 배웁니다. 셋째, sklearn으로 직접 구현해봅니다."

---

### Part 1: 앙상블 학습 개념 (5분)

#### 슬라이드 5-6: 앙상블이란

> "앙상블은 음악에서 온 용어입니다. 합주단이라는 뜻이죠. 여러 악기가 모여서 하나의 곡을 연주하는 것처럼, 여러 모델이 모여서 하나의 예측을 합니다."

> "핵심 아이디어는 간단합니다. 한 명의 전문가보다 여러 전문가의 의견을 종합하면 더 좋은 결정을 내릴 수 있다는 거죠."

---

#### 슬라이드 7-8: 집단 지성

> "유명한 실험이 있습니다. 병에 담긴 젤리빈 개수 맞추기 실험이죠. 개인의 추측은 오차가 크지만, 모든 사람의 추측을 평균내면 실제 값에 아주 가깝습니다."

> "왜 그럴까요? 각 사람이 다른 방향으로 틀리기 때문입니다. 누구는 많다고 하고, 누구는 적다고 하는데, 평균내면 오류가 상쇄됩니다."

> "머신러닝도 마찬가지입니다. 각 모델이 서로 다른 오류를 내면, 결합했을 때 오류가 줄어듭니다."

---

#### 슬라이드 9-10: 앙상블 조건

> "단, 조건이 있습니다. 각 모델이 어느 정도는 정확해야 하고, 서로 다른 오류를 내야 합니다."

> "똑같은 모델 100개를 만들면 의미가 없어요. 모두 같은 곳에서 틀리니까요."

> "그래서 랜덤포레스트는 '랜덤'을 사용합니다. 각 트리가 다르게 만들어지도록요."

---

#### 슬라이드 11-12: 배깅 vs 부스팅

> "앙상블 방법은 크게 배깅과 부스팅이 있습니다."

> "배깅은 모델들을 병렬로 학습합니다. 서로 독립이에요. 랜덤포레스트가 배깅 기반입니다."

> "부스팅은 순차적으로 학습합니다. 이전 모델이 틀린 부분에 집중해서 다음 모델을 만들죠. XGBoost, LightGBM이 여기에 속합니다."

> "오늘은 배깅 기반인 랜덤포레스트를 배웁니다."

---

### Part 2: 랜덤포레스트 원리 (5분)

#### 슬라이드 13-14: 랜덤포레스트란

> "랜덤포레스트는 이름 그대로 '랜덤한 숲'입니다. 의사결정나무 여러 개를 만들고, 결과를 투표로 결합합니다."

> "핵심 질문은 '어떻게 트리들을 다르게 만드느냐'입니다. 여기서 두 가지 랜덤이 사용됩니다."

---

#### 슬라이드 15-17: 두 가지 랜덤

> "첫 번째는 데이터 랜덤입니다. 부트스트랩 샘플링이라고 해서, 원본 데이터에서 복원 추출로 새 데이터셋을 만듭니다."

> "예를 들어 [A, B, C, D, E] 데이터가 있으면, 복원 추출로 [A, A, C, D, E]처럼 일부가 중복되는 새 데이터셋을 만듭니다. 각 트리마다 다른 데이터셋으로 학습하는 거죠."

> "두 번째는 특성 랜덤입니다. 각 분할마다 전체 특성이 아니라 일부 특성만 랜덤하게 선택해서 고려합니다. 이러면 트리들이 더 다양해집니다."

---

#### 슬라이드 18-20: 예측 방법

> "학습이 끝나면 예측은 투표로 합니다. 분류에서는 다수결이에요."

> "트리 1이 정상, 트리 2가 불량, 트리 3이 정상이면, 2:1로 정상이 됩니다."

> "회귀에서는 평균을 냅니다. 트리 1이 1200, 트리 2가 1150, 트리 3이 1200이면, 평균 1183이 됩니다."

---

#### 슬라이드 21-22: OOB 점수

> "재미있는 특성이 있습니다. OOB, Out-of-Bag 점수입니다."

> "부트스트랩 샘플링 특성상 약 37%의 데이터가 각 트리 학습에서 빠집니다. 이 데이터로 해당 트리를 평가하면, 별도 테스트 세트 없이도 성능을 추정할 수 있습니다."

> "oob_score=True로 설정하면 자동으로 계산해줍니다."

---

### Part 3: sklearn 실습 (5분)

#### 슬라이드 23-25: RandomForestClassifier

> "sklearn에서 RandomForestClassifier를 사용합니다."

```python
from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(
    n_estimators=100,  # 트리 개수
    max_depth=10,      # 깊이 제한
    random_state=42,
    n_jobs=-1          # 병렬 처리
)
```

> "n_estimators가 가장 중요합니다. 트리 개수예요. 보통 100~200이면 충분합니다."

> "n_jobs=-1은 모든 CPU 코어를 사용하라는 뜻입니다. 병렬 학습이 가능해서 빨라집니다."

---

#### 슬라이드 26-28: 학습과 예측

> "사용법은 의사결정나무와 동일합니다. fit, predict, score 패턴이죠."

```python
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
accuracy = model.score(X_test, y_test)
```

> "특성 중요도도 feature_importances_로 동일하게 볼 수 있습니다. 랜덤포레스트의 특성 중요도가 더 신뢰할 만합니다. 여러 트리의 평균이니까요."

---

#### 슬라이드 29-30: 의사결정나무와 비교

> "같은 데이터로 의사결정나무와 랜덤포레스트를 비교하면, 랜덤포레스트가 대체로 테스트 정확도가 높고 안정적입니다."

> "특히 여러 번 실험해보면 의사결정나무는 분산이 크고, 랜덤포레스트는 분산이 작습니다. 더 일관된 결과를 내는 거죠."

---

### 실습편 (11분)

#### 슬라이드 31-32: 실습 개요

> "이제 직접 실습합니다. 의사결정나무와 랜덤포레스트를 비교하고, 최적 파라미터를 찾아봅니다."

---

#### 슬라이드 33-35: 데이터 준비

> "제조 불량 데이터를 생성합니다. 이번에는 특성을 5개로 늘려봤습니다."

```python
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

np.random.seed(42)
n = 2000

df = pd.DataFrame({
    'temperature': np.random.normal(85, 5, n),
    'humidity': np.random.normal(50, 10, n),
    'speed': np.random.normal(100, 15, n),
    'pressure': np.random.normal(1.0, 0.1, n),
    'vibration': np.random.normal(5, 1, n),
})
# 불량 패턴 생성
...
```

---

#### 슬라이드 36-38: 모델 비교

> "의사결정나무와 랜덤포레스트를 동시에 학습하고 비교합니다."

```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

dt = DecisionTreeClassifier(max_depth=10, random_state=42)
rf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)

dt.fit(X_train, y_train)
rf.fit(X_train, y_train)

print(f"의사결정나무: {dt.score(X_test, y_test):.1%}")
print(f"랜덤포레스트: {rf.score(X_test, y_test):.1%}")
```

> "랜덤포레스트가 보통 2~5%p 정도 높은 정확도를 보입니다."

---

#### 슬라이드 39-41: 안정성 실험

> "여러 번 데이터를 나누어 실험하면 안정성 차이가 명확히 보입니다."

```python
for i in range(10):
    X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2)
    dt.fit(X_tr, y_tr)
    rf.fit(X_tr, y_tr)
    # 점수 기록...
```

> "의사결정나무는 표준편차가 크고, 랜덤포레스트는 작습니다. 랜덤포레스트가 더 안정적이라는 증거죠."

---

#### 슬라이드 42-44: n_estimators 실험

> "트리 개수를 바꿔가며 성능을 확인합니다."

> "10개에서는 성능이 낮다가, 50~100개에서 크게 올라가고, 그 이후로는 거의 포화됩니다."

> "시간과 메모리를 고려하면 100~200개가 적당합니다."

---

#### 슬라이드 45-47: 특성 중요도

> "랜덤포레스트의 특성 중요도를 의사결정나무와 비교해봅니다."

> "랜덤포레스트의 특성 중요도가 더 안정적입니다. 여러 트리의 평균이니까 개별 트리의 편향이 줄어듭니다."

---

### 정리 (2분)

#### 슬라이드 48-49: 핵심 정리

> "오늘 배운 내용을 정리합니다."

> "앙상블은 여러 모델을 결합해서 성능을 높이는 방법입니다."

> "랜덤포레스트는 배깅 기반으로, 부트스트랩 샘플링과 특성 랜덤 선택으로 다양한 트리를 만들고 투표로 결합합니다."

> "의사결정나무보다 정확도 높고, 안정적이며, 별도 튜닝 없이도 좋은 성능을 냅니다."

---

#### 슬라이드 50-51: 다음 차시 예고

> "다음 시간에는 분류에서 회귀로 넘어갑니다. 숫자를 예측하는 선형회귀와 다항회귀를 배웁니다."

> "오늘 수업 마무리합니다. 수고하셨습니다!"

---

## ❓ 예상 질문 및 답변

### Q1: 트리 개수가 많을수록 좋은 건가요?

> "어느 정도까지는 그렇습니다. 하지만 100~200개 이후로는 성능 향상이 미미하고, 학습 시간과 메모리만 늘어납니다. 보통 100개로 시작해서 필요하면 늘리세요."

### Q2: 랜덤포레스트도 과대적합되나요?

> "의사결정나무보다 과대적합 저항력이 강하지만, max_depth를 제한하지 않으면 과대적합될 수 있습니다. max_depth=15~20 정도로 제한하거나, min_samples_leaf를 설정하세요."

### Q3: OOB 점수와 테스트 점수 중 어떤 걸 봐야 하나요?

> "둘 다 참고하세요. OOB는 별도 검증 세트가 없을 때 유용하고, 일반적으로 테스트 점수와 비슷하게 나옵니다. 최종 평가는 테스트 세트로 하는 게 좋습니다."

### Q4: 랜덤포레스트가 항상 의사결정나무보다 좋나요?

> "대부분 그렇지만, 해석이 중요한 경우나 아주 빠른 예측이 필요한 경우에는 단일 의사결정나무가 나을 수 있습니다. 상황에 따라 선택하세요."

### Q5: XGBoost와 랜덤포레스트 중 어떤 게 좋나요?

> "일반적으로 XGBoost가 성능이 조금 더 좋습니다. 하지만 랜덤포레스트는 튜닝 없이도 좋은 성능을 내고, 과대적합에 강합니다. 빠른 프로토타이핑에는 랜덤포레스트, 최고 성능이 필요하면 XGBoost를 고려하세요."

---

## 📚 참고 자료

### 공식 문서
- [sklearn RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)
- [sklearn Ensemble methods](https://scikit-learn.org/stable/modules/ensemble.html)

### 관련 차시
- 12차시: 분류 모델 - 의사결정나무
- 14차시: 예측 모델 - 선형/다항 회귀
- 15차시: 모델 평가와 반복 검증

---

## ✅ 체크리스트

수업 전:
- [ ] sklearn 설치 확인
- [ ] 예제 코드 테스트
- [ ] CPU 코어 수 확인 (n_jobs 설명용)

수업 중:
- [ ] 앙상블 = 집단 지성 개념 설명
- [ ] 부트스트랩 샘플링 설명
- [ ] 특성 랜덤 선택 설명
- [ ] 의사결정나무와 비교 실습
- [ ] n_estimators 실험

수업 후:
- [ ] 실습 코드 배포
- [ ] 회귀 예고
