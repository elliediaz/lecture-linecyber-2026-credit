# [20차시] 딥러닝 입문: 신경망 기초 - 강사 스크립트

## 수업 개요

| 항목 | 내용 |
|------|------|
| 차시 | 20차시 |
| 주제 | 딥러닝 입문: 신경망 기초 |
| 시간 | 30분 (이론 15분 + 실습 13분 + 정리 2분) |
| 학습 목표 | 인공 뉴런, 신경망 구조, 순전파/역전파 이해 |

---

## 학습 목표

1. 인공 뉴런의 구조와 동작 원리를 설명한다
2. 신경망의 층 구조와 파라미터를 이해한다
3. 순전파와 역전파의 개념을 설명한다

---

## 시간 배분

| 구간 | 시간 | 내용 |
|------|------|------|
| 도입 | 2분 | 복습 및 학습목표 |
| 대주제 1 | 5분 | 인공 뉴런의 개념 |
| 대주제 2 | 5분 | 신경망 구조 |
| 대주제 3 | 5분 | 순전파와 역전파 |
| 실습 | 11분 | NumPy로 신경망 구현 |
| 정리 | 2분 | 요약 및 다음 차시 예고 |

---

## 상세 스크립트

### 도입부 (2분)

#### 슬라이드 1-2: 복습

> "지난 시간에 시계열 예측 모델을 배웠습니다. RandomForest로 생산량을 예측하고 MAE, MAPE로 평가했죠."

> "오늘부터 딥러닝을 시작합니다. 먼저 신경망의 기초를 배우고, 다음 시간에 실제로 Keras로 모델을 만들어봅니다."

---

#### 슬라이드 3-4: 왜 딥러닝인가

> "머신러닝으로 못 풀거나 어려운 문제가 있습니다. 이미지, 자연어, 복잡한 패턴이 그렇죠."

> "딥러닝은 특성을 자동으로 학습합니다. 우리가 특성 엔지니어링을 덜 해도 되는 거예요."

> "제조 분야에서는 불량 이미지 검출, 센서 이상 탐지 등에 딥러닝이 많이 쓰입니다."

---

#### 슬라이드 5-6: 학습 목표

> "오늘의 학습 목표는 세 가지입니다. 인공 뉴런의 구조, 신경망의 층 구조, 순전파와 역전파를 이해합니다."

> "수학적으로 완벽하게 이해할 필요는 없어요. 직관적으로 '어떻게 동작하는지'를 알면 됩니다."

---

### 대주제 1: 인공 뉴런의 개념 (5분)

#### 슬라이드 7-9: 생물학적 뉴런

> "딥러닝은 뇌의 신경세포에서 영감을 받았습니다."

> "생물학적 뉴런은 여러 입력을 받아서 합치고, 특정 조건에서 신호를 전달합니다. 이걸 수학으로 모델링한 게 인공 뉴런이에요."

---

#### 슬라이드 10-12: 퍼셉트론 개념

> "인공 뉴런, 또는 퍼셉트론의 구조를 보겠습니다."

> "입력 x1, x2, x3이 있고, 각각에 가중치 w1, w2, w3이 곱해집니다. 이걸 다 더하고 편향 b를 더합니다."

```
z = w1*x1 + w2*x2 + w3*x3 + b
```

> "여기에 활성화 함수를 통과시켜서 출력을 만듭니다."

---

#### 슬라이드 13-15: 가중치와 편향

> "가중치(weight)는 각 입력의 중요도입니다. 가중치가 크면 그 입력이 더 중요하다는 뜻이에요."

> "편향(bias)은 활성화 기준점을 조절합니다. 절편이라고 생각하시면 됩니다."

> "학습은 이 가중치와 편향을 조절하는 과정입니다. 정답에 가까워지도록 계속 조정하는 거예요."

---

#### 슬라이드 16-18: 활성화 함수 종류

> "활성화 함수는 비선형성을 추가합니다. 활성화 함수가 없으면 층을 아무리 깊게 쌓아도 선형 모델과 같아요."

> "**Sigmoid**: 0~1 사이 출력. 확률로 해석 가능. 이진 분류 출력층에 사용."

```python
def sigmoid(z):
    return 1 / (1 + np.exp(-z))
```

> "**ReLU**: 0보다 크면 그대로, 작으면 0. 은닉층에서 가장 많이 사용. 계산이 빠르고 학습이 잘 됩니다."

```python
def relu(z):
    return np.maximum(0, z)
```

> "**Softmax**: 다중 클래스 분류 출력층에 사용. 모든 출력의 합이 1이 됩니다."

---

### 대주제 2: 신경망 구조 (5분)

#### 슬라이드 19-21: 층(Layer) 구조

> "신경망은 뉴런을 층으로 쌓은 구조입니다."

> "**입력층**: 데이터가 들어오는 곳. 특성 개수만큼 노드가 있습니다."

> "**은닉층**: 입력과 출력 사이에 있는 층. 여러 개 쌓을 수 있어요. 층이 깊을수록 복잡한 패턴을 학습합니다."

> "**출력층**: 최종 결과를 내는 층. 분류면 클래스 수만큼, 회귀면 1개 노드."

---

#### 슬라이드 22-24: 완전연결층 (Dense)

> "완전연결층, 또는 Dense층은 이전 층의 모든 노드가 다음 층의 모든 노드와 연결된 구조입니다."

> "가장 기본적인 층이에요. Keras에서 Dense라고 부릅니다."

```python
from keras.layers import Dense
model.add(Dense(64, activation='relu'))
```

---

#### 슬라이드 25-27: MLP 구조

> "MLP는 Multi-Layer Perceptron, 다층 퍼셉트론입니다. 은닉층이 1개 이상인 신경망이에요."

> "예를 들어, 입력 4개 → 은닉 8개 → 출력 3개 구조라면:"

```
입력층: 4 노드
은닉층: 8 노드 (ReLU)
출력층: 3 노드 (Softmax)
```

> "파라미터 수를 계산해볼까요? 입력→은닉: 4×8 + 8 = 40개, 은닉→출력: 8×3 + 3 = 27개. 총 67개입니다."

---

#### 슬라이드 28-30: 파라미터 수 계산

> "파라미터 수는 (이전 층 노드 수 × 현재 층 노드 수) + 편향 개수입니다."

> "층을 깊게 쌓거나 노드 수를 늘리면 파라미터가 급격히 증가합니다. 그래서 복잡한 패턴을 학습할 수 있지만, 과적합 위험도 커져요."

---

### 대주제 3: 순전파와 역전파 (5분)

#### 슬라이드 31-33: 순전파 개념

> "순전파(Forward Propagation)는 입력이 출력까지 전달되는 과정입니다."

> "입력 → 첫 번째 은닉층 → 두 번째 은닉층 → 출력 순서로 계산합니다."

> "각 층에서 가중치 곱하고 편향 더하고 활성화 함수 통과. 이걸 반복합니다."

```python
# 층 1
z1 = X @ W1 + b1
a1 = relu(z1)

# 층 2
z2 = a1 @ W2 + b2
y_pred = sigmoid(z2)
```

---

#### 슬라이드 34-36: 손실 함수

> "예측값과 실제값이 얼마나 다른지 측정하는 게 손실 함수입니다."

> "**MSE (Mean Squared Error)**: 회귀 문제에 사용."

```python
loss = np.mean((y_true - y_pred)**2)
```

> "**Binary Cross-Entropy**: 이진 분류에 사용."

> "**Categorical Cross-Entropy**: 다중 분류에 사용."

> "학습의 목표는 이 손실을 최소화하는 것입니다."

---

#### 슬라이드 37-39: 역전파 개념

> "역전파(Backpropagation)는 손실을 줄이기 위해 가중치를 업데이트하는 방법입니다."

> "손실을 각 가중치로 미분합니다. '이 가중치를 얼마나 바꾸면 손실이 줄어들까?'를 계산하는 거예요."

> "출력층에서 시작해서 입력층 방향으로 거슬러 올라갑니다. 그래서 역전파예요."

---

#### 슬라이드 40-42: 경사하강법

> "경사하강법(Gradient Descent)은 손실을 줄이는 방향으로 가중치를 조금씩 이동시킵니다."

```python
W = W - learning_rate * gradient
```

> "학습률(learning rate)은 한 번에 얼마나 이동할지를 결정합니다. 너무 크면 최적점을 지나치고, 너무 작으면 학습이 느려요."

> "경사(gradient)는 기울기입니다. 기울기가 가리키는 반대 방향으로 이동하면 손실이 줄어듭니다."

---

#### 슬라이드 43-45: 에포크와 배치

> "**에포크(Epoch)**: 전체 데이터를 한 번 다 학습하는 것. 에포크 10이면 전체 데이터를 10번 반복 학습."

> "**배치(Batch)**: 한 번에 학습하는 데이터 개수. 배치 크기 32면 32개씩 묶어서 학습."

> "배치가 작으면 업데이트가 자주 일어나고, 크면 안정적이지만 메모리를 많이 씁니다."

---

### 실습편 (11분)

#### 슬라이드 46-48: 실습 준비

> "NumPy로 간단한 신경망을 직접 구현해봅시다. Keras 없이 원리를 이해하는 게 목표입니다."

```python
import numpy as np

# 시드 고정
np.random.seed(42)
```

---

#### 슬라이드 49-51: 활성화 함수 구현

> "먼저 활성화 함수를 구현합니다."

```python
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def relu(z):
    return np.maximum(0, z)

def relu_derivative(z):
    return (z > 0).astype(float)
```

> "relu_derivative는 역전파에서 사용합니다. ReLU는 양수면 기울기 1, 음수면 0이에요."

---

#### 슬라이드 52-54: 순전파 구현

> "2층 신경망의 순전파를 구현합니다."

```python
def forward(X, W1, b1, W2, b2):
    # 은닉층
    z1 = X @ W1 + b1
    a1 = relu(z1)

    # 출력층
    z2 = a1 @ W2 + b2
    a2 = sigmoid(z2)

    return z1, a1, z2, a2
```

> "@ 연산자는 행렬 곱입니다. np.dot과 같아요."

---

#### 슬라이드 55-57: 손실 계산

> "Binary Cross-Entropy 손실을 계산합니다."

```python
def compute_loss(y_true, y_pred):
    epsilon = 1e-8  # log(0) 방지
    loss = -np.mean(
        y_true * np.log(y_pred + epsilon) +
        (1 - y_true) * np.log(1 - y_pred + epsilon)
    )
    return loss
```

---

#### 슬라이드 58-60: 역전파 구현

> "역전파로 기울기를 계산합니다."

```python
def backward(X, y, z1, a1, z2, a2, W2):
    m = X.shape[0]

    # 출력층 기울기
    dz2 = a2 - y
    dW2 = (a1.T @ dz2) / m
    db2 = np.mean(dz2, axis=0, keepdims=True)

    # 은닉층 기울기
    da1 = dz2 @ W2.T
    dz1 = da1 * relu_derivative(z1)
    dW1 = (X.T @ dz1) / m
    db1 = np.mean(dz1, axis=0, keepdims=True)

    return dW1, db1, dW2, db2
```

---

#### 슬라이드 61-63: 학습 루프

> "학습 루프를 구현합니다."

```python
# 초기화
W1 = np.random.randn(input_size, hidden_size) * 0.5
b1 = np.zeros((1, hidden_size))
W2 = np.random.randn(hidden_size, output_size) * 0.5
b2 = np.zeros((1, output_size))

learning_rate = 0.1

for epoch in range(1000):
    # 순전파
    z1, a1, z2, a2 = forward(X, W1, b1, W2, b2)

    # 손실 계산
    loss = compute_loss(y, a2)

    # 역전파
    dW1, db1, dW2, db2 = backward(X, y, z1, a1, z2, a2, W2)

    # 가중치 업데이트
    W1 -= learning_rate * dW1
    b1 -= learning_rate * db1
    W2 -= learning_rate * dW2
    b2 -= learning_rate * db2
```

---

#### 슬라이드 64-66: 학습 결과

> "XOR 문제를 학습시켜봅니다. XOR은 선형으로는 풀 수 없지만 신경망은 풀 수 있어요."

```python
# XOR 데이터
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([[0], [1], [1], [0]])

# 학습 후 예측
_, _, _, y_pred = forward(X, W1, b1, W2, b2)
print(y_pred.round())
# [[0], [1], [1], [0]]
```

> "은닉층 덕분에 비선형 결정 경계를 학습할 수 있습니다."

---

### 정리 (2분)

#### 슬라이드 67-68: 핵심 정리

> "오늘 배운 내용을 정리합니다."

> "**인공 뉴런**: 입력에 가중치를 곱하고 편향을 더한 후 활성화 함수를 통과. 가중치는 중요도, 편향은 기준점."

> "**신경망 구조**: 입력층 → 은닉층(들) → 출력층. 층이 깊을수록 복잡한 패턴 학습. 파라미터 수 = (이전 노드 × 현재 노드) + 편향."

> "**순전파/역전파**: 순전파로 예측, 손실 계산, 역전파로 기울기 계산, 경사하강법으로 가중치 업데이트."

---

#### 슬라이드 69-70: 다음 차시 예고

> "다음 시간에는 Keras로 실제 MLP를 만들어서 품질 예측을 해봅니다. 오늘 배운 원리가 Keras 내부에서 자동으로 돌아가는 거예요."

> "오늘 수업 마무리합니다. 수고하셨습니다!"

---

## 예상 질문 및 답변

### Q1: 활성화 함수를 왜 써야 하나요?

> "활성화 함수 없이 층을 쌓으면 결국 하나의 선형 변환과 같습니다. y = W₁(W₂x) = (W₁W₂)x = Wx. 비선형 함수가 있어야 복잡한 패턴을 학습할 수 있어요."

### Q2: ReLU와 Sigmoid 중 뭘 써야 하나요?

> "은닉층에는 ReLU를 권장합니다. 계산이 빠르고 기울기 소실 문제가 적어요. 출력층에서 확률이 필요하면 Sigmoid(이진 분류)나 Softmax(다중 분류)를 씁니다."

### Q3: 학습률은 어떻게 정하나요?

> "보통 0.001~0.1 사이에서 시작합니다. 너무 크면 손실이 발산하고, 너무 작으면 학습이 느려요. Keras에서는 Adam 같은 옵티마이저가 자동으로 조절해줍니다."

### Q4: 에포크를 많이 하면 좋은가요?

> "너무 많으면 과적합됩니다. 검증 손실을 모니터링하면서 더 이상 개선이 없으면 멈추는 게 좋아요. Early Stopping이라고 합니다."

### Q5: 왜 NumPy로 구현하는 건가요? Keras 쓰면 되잖아요?

> "원리를 이해하기 위해서입니다. 순전파, 역전파가 어떻게 동작하는지 알면 Keras를 더 잘 쓸 수 있어요. 문제가 생겼을 때 디버깅도 쉬워집니다."

### Q6: 딥러닝과 머신러닝의 차이가 뭔가요?

> "딥러닝은 머신러닝의 한 분야입니다. 신경망이 여러 층으로 깊어진 것이 딥러닝이에요. 특성을 자동으로 학습하는 게 장점입니다."

---

## 참고 자료

### 공식 문서
- [Keras Dense Layer](https://keras.io/api/layers/core_layers/dense/)
- [NumPy Matrix Operations](https://numpy.org/doc/stable/reference/routines.linalg.html)

### 추천 자료
- 3Blue1Brown - Neural Networks 시리즈 (YouTube)
- Deep Learning (Ian Goodfellow) - 2장, 6장

### 관련 차시
- 18차시: 시계열 예측 모델
- 20차시: 딥러닝 실습 - MLP로 품질 예측

---

## 체크리스트

수업 전:
- [ ] NumPy 신경망 코드 테스트
- [ ] XOR 문제 학습 결과 확인
- [ ] 활성화 함수 시각화 준비

수업 중:
- [ ] 가중치와 편향 개념 강조
- [ ] 활성화 함수의 역할 설명
- [ ] 순전파 → 손실 → 역전파 흐름 반복 강조
- [ ] 수식보다 직관적 이해 우선

수업 후:
- [ ] 실습 코드 배포
- [ ] Keras 예고
