---
marp: true
theme: default
paginate: true
backgroundColor: #fff
---

<!-- _class: lead -->
# [20차시] 딥러닝 입문: 신경망 기초

## 공공데이터 AI 예측 모델 개발

---

# 학습 목표

1. **인공 뉴런의 개념**을 이해한다
2. **신경망 구조**를 파악한다
3. **순전파와 역전파**를 이해한다

---

# 목차

## 대주제 1: 인공 뉴런의 개념
## 대주제 2: 신경망 구조
## 대주제 3: 순전파와 역전파
## 실습편: NumPy로 신경망 구현

---

<!-- _class: lead -->
# 대주제 1
## 인공 뉴런의 개념을 이해한다

---

# 머신러닝 vs 딥러닝

| 구분 | 머신러닝 | 딥러닝 |
|------|----------|--------|
| 모델 | RF, SVM, LR | 신경망 |
| 특성 추출 | 수동 | 자동 |
| 데이터 양 | 적어도 가능 | 많아야 효과적 |
| 계산 자원 | CPU 가능 | GPU 권장 |
| 해석 | 비교적 쉬움 | 블랙박스 |

---

# 딥러닝이란?

## 정의
- **깊은(Deep)** 인공 신경망을 사용한 학습
- 여러 층(Layer)을 쌓아 복잡한 패턴 학습

## 왜 "딥"인가?
- 층이 2개 이상 → Deep
- 층이 많을수록 복잡한 표현 가능

---

# 생물학적 뉴런

```
                  수상돌기
                    │
                    ↓
[입력 신호] → [세포체] → [축삭] → [출력]
                    │
             역치 초과 시 발화
```

## 동작 원리
1. 수상돌기로 신호 수신
2. 세포체에서 신호 합산
3. 역치 초과 시 축삭으로 출력

---

# 인공 뉴런 (Perceptron)

```
   x₁ ──w₁──┐
   x₂ ──w₂──┤
   x₃ ──w₃──┼──→ Σ ──→ f(z) ──→ y
      ...   │
   xₙ ──wₙ──┘
            └── +b (bias)
```

## 수식
**z = w₁x₁ + w₂x₂ + ... + wₙxₙ + b**
**y = f(z)** (활성화 함수)

---

# 인공 뉴런의 구성요소

| 요소 | 기호 | 설명 |
|------|------|------|
| 입력 | x₁, x₂, ... | 특성 값 |
| 가중치 | w₁, w₂, ... | 입력의 중요도 |
| 편향 | b | 기준점 조정 |
| 합계 | z | 가중합 + 편향 |
| 활성화 함수 | f | 비선형 변환 |
| 출력 | y | 최종 결과 |

---

# 가중치의 의미

```python
# 온도와 압력으로 불량 예측
z = w₁ × 온도 + w₂ × 압력 + b

# w₁ = 0.8, w₂ = 0.2
# → 온도가 4배 더 중요!
```

## 학습 = 최적의 가중치 찾기
- 데이터를 통해 w, b 조정
- 예측 오차 최소화

---

# 활성화 함수의 필요성

## 없으면?
```
z = w₁x₁ + w₂x₂ + b  (선형)
```
→ 층을 아무리 쌓아도 선형!

## 있으면?
```
y = f(w₁x₁ + w₂x₂ + b)  (비선형)
```
→ 복잡한 패턴 학습 가능!

---

# 주요 활성화 함수

| 함수 | 수식 | 출력 범위 |
|------|------|----------|
| Sigmoid | 1/(1+e⁻ˣ) | 0~1 |
| Tanh | (eˣ-e⁻ˣ)/(eˣ+e⁻ˣ) | -1~1 |
| ReLU | max(0, x) | 0~∞ |
| Softmax | eˣⁱ/Σeˣʲ | 0~1 (합=1) |

---

# Sigmoid 함수

```
      1 ─────────────────
                     ╱
    0.5 ───────────╱
                 ╱
      0 ───────╱─────────
           -∞    0    ∞
```

## 특징
- 출력: 0~1 (확률 해석)
- 문제: 기울기 소실 (Vanishing Gradient)

---

# ReLU 함수

```
    y │      ╱
      │     ╱
      │    ╱
      │   ╱
    ──┼──╱───────────── x
      │ 0
```

**f(x) = max(0, x)**

## 특징
- 간단하고 빠름
- 기울기 소실 완화
- **현재 가장 많이 사용**

---

# 활성화 함수 선택 가이드

| 위치 | 권장 함수 |
|------|----------|
| 은닉층 | ReLU (또는 Leaky ReLU) |
| 출력층 (이진 분류) | Sigmoid |
| 출력층 (다중 분류) | Softmax |
| 출력층 (회귀) | 없음 (Linear) |

---

<!-- _class: lead -->
# 대주제 2
## 신경망 구조를 파악한다

---

# 신경망의 층 구조

```
  입력층        은닉층1       은닉층2        출력층
  ┌───┐        ┌───┐        ┌───┐        ┌───┐
  │ ○ │───────▶│ ○ │───────▶│ ○ │───────▶│ ○ │
  │ ○ │───────▶│ ○ │───────▶│ ○ │───────▶│ ○ │
  │ ○ │───────▶│ ○ │───────▶│ ○ │───────▶└───┘
  │ ○ │───────▶│ ○ │───────▶└───┘
  └───┘        └───┘
   (4)          (4)          (3)          (2)
```

---

# 층의 종류

## 입력층 (Input Layer)
- 특성 값을 받는 층
- 뉴런 수 = 특성 수

## 은닉층 (Hidden Layer)
- 입력과 출력 사이의 층
- 깊이 = 은닉층 개수

## 출력층 (Output Layer)
- 최종 예측 출력
- 뉴런 수 = 클래스 수 (분류) 또는 1 (회귀)

---

# 완전 연결 층 (Dense Layer)

```
  이전 층         다음 층
   ○───────────────○
    ╲             ╱
     ╲           ╱
      ╲─────────○
       ╲       ╱
        ╲─────○
         ╲   ╱
          ╲─○
```

## 특징
- 모든 뉴런이 서로 연결
- FC Layer, Dense Layer라고 함

---

# 신경망의 파라미터 수

## 예시: 4 → 3 → 2 신경망

```
입력층 → 은닉층: 4×3 + 3 = 15
은닉층 → 출력층: 3×2 + 2 = 8
────────────────────────
총 파라미터: 23개
```

## 공식
**파라미터 수 = (입력 × 출력) + 출력**
(가중치 행렬 + 편향 벡터)

---

# MLP (Multi-Layer Perceptron)

## 정의
- 여러 층으로 구성된 퍼셉트론
- 가장 기본적인 신경망

## 구조
```
입력 → [Dense + ReLU] × n → [Dense] → 출력
```

---

# 신경망 설계 고려사항

| 요소 | 고려사항 |
|------|----------|
| 층 수 | 깊을수록 복잡한 패턴 (과적합 주의) |
| 뉴런 수 | 많을수록 표현력 증가 |
| 활성화 함수 | 은닉층은 ReLU |
| 출력층 | 문제 유형에 맞게 |

---

# 일반적인 MLP 구조

## 분류 문제
```
입력(특성수) → Dense(64, ReLU) → Dense(32, ReLU)
             → Dense(클래스수, Softmax)
```

## 회귀 문제
```
입력(특성수) → Dense(64, ReLU) → Dense(32, ReLU)
             → Dense(1, Linear)
```

---

<!-- _class: lead -->
# 대주제 3
## 순전파와 역전파를 이해한다

---

# 신경망 학습 과정

```
┌─────────────────────────────────────────┐
│ 1. 순전파: 입력 → 출력 계산              │
│ 2. 손실 계산: 예측 vs 실제 비교          │
│ 3. 역전파: 기울기(gradient) 계산         │
│ 4. 가중치 업데이트: w = w - η × ∂L/∂w  │
│ 5. 반복 (epoch)                         │
└─────────────────────────────────────────┘
```

---

# 순전파 (Forward Propagation)

```
입력 x → 은닉층 → 출력 ŷ

z₁ = W₁x + b₁
a₁ = ReLU(z₁)
z₂ = W₂a₁ + b₂
ŷ = Softmax(z₂)
```

## 의미
- 입력을 받아 출력을 계산하는 과정
- 예측값 생성

---

# 손실 함수 (Loss Function)

## 정의
- 예측과 실제의 **차이**를 수치화

| 문제 | 손실 함수 |
|------|----------|
| 이진 분류 | Binary Cross-Entropy |
| 다중 분류 | Categorical Cross-Entropy |
| 회귀 | MSE (Mean Squared Error) |

---

# 손실 함수 예시

## MSE (회귀)
```
L = (1/n) × Σ(yᵢ - ŷᵢ)²
```

## Cross-Entropy (분류)
```
L = -Σ yᵢ × log(ŷᵢ)
```

## 목표: L을 최소화하는 w, b 찾기

---

# 역전파 (Backpropagation)

## 목적
- 손실 함수의 기울기 계산
- 각 가중치가 손실에 얼마나 영향?

## 과정
```
출력 ← 은닉층 ← 입력

∂L/∂W₂ = ... (출력층 기울기)
∂L/∂W₁ = ... (은닉층 기울기, 연쇄법칙)
```

---

# 경사하강법 (Gradient Descent)

```
    손실 L
      │    ╲
      │     ╲
      │      ○ → 현재 위치
      │       ╲
      │        ╲─→ 기울기 방향
      │         ╲
      │──────────╲──── w
                  ● 최저점
```

## 업데이트 규칙
**w = w - η × (∂L/∂w)**

---

# 학습률 (Learning Rate)

## 정의
- 한 번에 얼마나 움직일지
- η (eta)로 표기

## 영향
| 학습률 | 결과 |
|--------|------|
| 너무 작음 | 수렴 느림 |
| 적절함 | 안정적 수렴 |
| 너무 큼 | 발산, 불안정 |

---

# 에폭 (Epoch)

## 정의
- 전체 데이터를 한 번 학습 = 1 epoch

## 예시
```
데이터 1000개, batch_size=100
→ 1 epoch = 10번 업데이트
→ 100 epochs = 1000번 업데이트
```

## 선택
- 너무 적으면: 학습 부족
- 너무 많으면: 과적합

---

# 배치 크기 (Batch Size)

## 정의
- 한 번에 학습할 샘플 수

| 배치 크기 | 특징 |
|----------|------|
| 1 (SGD) | 빠른 업데이트, 불안정 |
| 32~256 | 일반적 선택 |
| 전체 (Batch GD) | 안정적, 느림 |

---

# 학습 곡선

```
  손실 │
       │╲
       │ ╲
       │  ╲───────────── 학습 손실
       │   ╲   ╱─────── 검증 손실
       │    ╲─╱  ← 최적점
       │     ╲
       └──────────────── epoch
```

## 과적합 신호
- 학습 손실 ↓, 검증 손실 ↑

---

<!-- _class: lead -->
# 실습편
## NumPy로 신경망 구현

---

# 실습 개요

## 목표
- NumPy로 간단한 신경망 구현
- 순전파/역전파 직접 체험

## 구조
- 입력: 2개
- 은닉층: 4개 (ReLU)
- 출력: 1개 (Sigmoid)

---

# 실습 1: 활성화 함수

```python
import numpy as np

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return x * (1 - x)

def relu(x):
    return np.maximum(0, x)

def relu_derivative(x):
    return (x > 0).astype(float)
```

---

# 실습 2: 신경망 클래스

```python
class SimpleNN:
    def __init__(self, input_size, hidden_size, output_size):
        # 가중치 초기화
        self.W1 = np.random.randn(input_size, hidden_size) * 0.5
        self.b1 = np.zeros((1, hidden_size))
        self.W2 = np.random.randn(hidden_size, output_size) * 0.5
        self.b2 = np.zeros((1, output_size))
```

---

# 실습 3: 순전파

```python
def forward(self, X):
    # 은닉층
    self.z1 = X @ self.W1 + self.b1
    self.a1 = relu(self.z1)

    # 출력층
    self.z2 = self.a1 @ self.W2 + self.b2
    self.a2 = sigmoid(self.z2)

    return self.a2
```

---

# 실습 4: 역전파

```python
def backward(self, X, y, learning_rate=0.1):
    m = X.shape[0]

    # 출력층 기울기
    dZ2 = self.a2 - y
    dW2 = (self.a1.T @ dZ2) / m
    db2 = np.sum(dZ2, axis=0, keepdims=True) / m

    # 은닉층 기울기
    dA1 = dZ2 @ self.W2.T
    dZ1 = dA1 * relu_derivative(self.a1)
    dW1 = (X.T @ dZ1) / m
    db1 = np.sum(dZ1, axis=0, keepdims=True) / m

    # 가중치 업데이트
    self.W2 -= learning_rate * dW2
    self.b2 -= learning_rate * db2
    self.W1 -= learning_rate * dW1
    self.b1 -= learning_rate * db1
```

---

# 실습 5: 학습 루프

```python
# 데이터 준비
X = np.array([[0,0], [0,1], [1,0], [1,1]])
y = np.array([[0], [1], [1], [0]])  # XOR

# 신경망 생성
nn = SimpleNN(input_size=2, hidden_size=4, output_size=1)

# 학습
for epoch in range(10000):
    output = nn.forward(X)
    nn.backward(X, y, learning_rate=0.5)

    if epoch % 2000 == 0:
        loss = np.mean((y - output)**2)
        print(f"Epoch {epoch}: Loss = {loss:.4f}")
```

---

# 실습 6: 결과 확인

```python
# 예측
predictions = nn.forward(X)
print("\n예측 결과:")
for i in range(len(X)):
    print(f"입력: {X[i]} → 예측: {predictions[i][0]:.4f} (실제: {y[i][0]})")

# 출력 예시:
# 입력: [0 0] → 예측: 0.0523 (실제: 0)
# 입력: [0 1] → 예측: 0.9412 (실제: 1)
# 입력: [1 0] → 예측: 0.9389 (실제: 1)
# 입력: [1 1] → 예측: 0.0612 (실제: 0)
```

---

# 핵심 정리

## 1. 인공 뉴런
- 입력 × 가중치 + 편향 → 활성화 함수
- ReLU가 은닉층에서 가장 많이 사용

## 2. 신경망 구조
- 입력층 → 은닉층(들) → 출력층
- MLP = 완전 연결 신경망

## 3. 학습 과정
- 순전파 → 손실 계산 → 역전파 → 가중치 업데이트

---

# 다음 차시 예고

## 20차시: 딥러닝 실습 - MLP 품질 예측
- Keras/TensorFlow 사용
- Sequential 모델 구축
- 제조 품질 예측 실습
- 학습 곡선 분석

---

<!-- _class: lead -->
# 수고하셨습니다!

## 실습 파일: `19_neural_network_basics.py`
