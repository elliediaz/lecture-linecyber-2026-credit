# [4차시] 공개 데이터셋 확보 및 데이터 생태계 이해 - 강사 스크립트

## 차시 정보
- **차시**: 4차시
- **제목**: 공개 데이터셋 확보 및 데이터 생태계 이해
- **시간**: 30분 (이론 15분 + 실습 15분)
- **대주제**: 1. 공공데이터의 정의와 활용 / 2. 주요 데이터 포털의 특성 / 3. 데이터셋 구조와 용어

---

## 도입부 (2분)

### 인사 및 수업 소개

> 여러분, 안녕하세요. 4차시 '공개 데이터셋 확보 및 데이터 생태계 이해' 수업을 시작하겠습니다.
>
> AI 모델을 만들려면 무엇이 가장 먼저 필요할까요? 바로 **데이터**입니다. 아무리 좋은 알고리즘도 데이터 없이는 의미가 없죠.
>
> 오늘은 AI 학습에 활용할 수 있는 공개 데이터를 어디서 어떻게 확보하는지 배워보겠습니다.

### 학습목표 제시

> 오늘 수업을 마치면 여러분은:
> - 첫째, 공공데이터의 법적 정의와 활용 목적을 이해하게 됩니다.
> - 둘째, 주요 데이터 포털들의 특성과 차이점을 비교할 수 있습니다.
> - 셋째, 데이터셋 구조와 용어를 정확히 알게 됩니다.
> - 넷째, 실제로 데이터를 다운로드하고 구조를 확인할 수 있습니다.

### 진행 순서 안내

> 오늘 수업은 이론 15분, 실습 15분으로 진행됩니다.
>
> 이론에서는 세 가지 주제를 다룹니다:
> 1. 공공데이터의 정의와 활용 - 5분
> 2. 주요 데이터 포털의 특성 - 5분
> 3. 데이터셋 구조와 용어 - 5분
>
> 그리고 실습에서 실제로 데이터를 다운로드해보겠습니다.

---

## 대주제 1: 공공데이터의 정의와 활용 (5분)

### 1.1 공공데이터의 법적 정의

> 자, 먼저 공공데이터가 정확히 뭔지 법적으로 알아봅시다.
>
> 공공데이터법 제2조를 보면, **"공공데이터란 공공기관이 법령에서 정하는 목적을 위해 생성하거나 취득하여 관리하는 전자적 방식으로 처리된 자료"**라고 정의하고 있습니다.
>
> 쉽게 말해서, 정부나 공공기관이 만들고 관리하는 디지털 데이터입니다.
>
> 핵심 요소가 세 가지 있는데요:
> - **생성 주체**: 정부, 지자체, 공기업 같은 공공기관
> - **형태**: CSV, Excel, API 같은 전자적 형태
> - **목적**: 통계, 행정, 공공 서비스 등 공공 목적

### 1.2 공공기관의 범위

> 공공데이터를 제공하는 기관은 어디가 있을까요?
>
> 국가기관, 지방자치단체, 공공기관, 지방공기업, 특수법인 등 **890개 이상**의 기관이 있습니다.
>
> 예를 들어:
> - 행정안전부, 산업통상자원부 같은 **정부부처**
> - 서울시, 경기도 같은 **지자체**
> - 한국전력, 건강보험공단 같은 **공공기관**
>
> 이런 기관들이 보유한 데이터를 우리가 무료로 활용할 수 있습니다.

### 1.3 공공데이터의 종류

> 공공데이터는 형태에 따라 네 가지로 나눕니다:
>
> 첫째, **파일 데이터** - CSV, Excel, JSON 파일을 다운로드
> 둘째, **오픈 API** - REST API로 실시간 조회
> 셋째, **표준 데이터** - 주소, 법인정보 같은 공통 규격 데이터
> 넷째, **링크드 데이터** - 지식 그래프 형태의 연결 데이터
>
> AI 학습에는 주로 파일 데이터(CSV)나 API를 많이 사용합니다.

### 1.4 공공데이터 제공 원칙

> 공공데이터는 5가지 원칙에 따라 제공됩니다:
>
> 1. **무료 제공** - 실비 외에는 무료입니다
> 2. **기계 판독** - CSV, JSON처럼 컴퓨터가 읽을 수 있는 형태
> 3. **최신성 유지** - 정기적으로 업데이트해야 합니다
> 4. **정확성 확보** - 오류를 최소화해야 합니다
> 5. **재이용 허용** - 상업적으로도 사용 가능합니다
>
> 이 덕분에 우리가 안심하고 비즈니스에 활용할 수 있습니다.

### 1.5 AI 학습에서의 활용

> AI 학습 데이터는 세 가지 소스에서 옵니다:
> - **공공데이터**: 인프라 정보, 통계, 환경 데이터
> - **민간데이터**: 기업의 고객, 거래 데이터
> - **자체 생성**: 센서 수집, 실험, 라벨링 데이터
>
> 공공데이터는 기반 정보를 제공하고, 민간/자체 데이터와 결합해서 AI 모델을 학습시킵니다.
>
> 예를 들어, 품질 예측 모델에서:
> - 기상청 날씨 데이터(공공) + 공장 센서 데이터(자체) = 날씨가 품질에 미치는 영향 분석

### 1.6 장단점

> 공공데이터의 장점은:
> - 무료 또는 저렴한 비용
> - 법적 근거가 있어서 신뢰성 높음
> - 대량 데이터 확보 가능
> - 상업적 이용 가능
>
> 주의할 점은:
> - 갱신 주기가 느릴 수 있음
> - 형식이 통일되지 않을 수 있음
> - AI용 라벨이 없는 경우가 많음
>
> 이런 특성을 이해하고 활용하면 됩니다.

---

## 대주제 2: 주요 데이터 포털의 특성 (5분)

### 2.1 국내 4대 데이터 포털

> 이제 실제로 데이터를 어디서 받을 수 있는지 알아봅시다.
>
> 국내 주요 포털 네 개를 소개합니다:
>
> 1. **공공데이터포털 (data.go.kr)** - 정부 공식, 45만+ 데이터셋
> 2. **AI 허브 (aihub.or.kr)** - AI 학습용 특화, 700+ 데이터셋
> 3. **Kaggle (kaggle.com)** - 글로벌 ML 커뮤니티, 20만+ 데이터셋
> 4. **통계청 KOSIS** - 국가 공식 통계, 100만+ 통계표

### 2.2 공공데이터포털

> 먼저 **공공데이터포털**입니다.
>
> - 운영: 행정안전부
> - 규모: 45만 개 이상의 데이터셋, 하루 3천만 건 이상의 API 호출
> - 특징: 대한민국 공식 공공데이터 게이트웨이
>
> 데이터 종류를 보면:
> - 파일 데이터가 38만 개 이상
> - 오픈 API가 7만 개 이상
> - 표준 데이터가 300개 이상
>
> 제조업에서는 산업통계, 품질안전정보, 에너지, 환경, 물류 데이터 등을 활용할 수 있습니다.

### 2.3 AI 허브

> 다음은 **AI 허브**입니다.
>
> - 운영: 한국지능정보사회진흥원(NIA)
> - 규모: 700개 이상의 AI 학습 데이터셋
> - 특징: **고품질 라벨링** 포함, AI 모델 학습 전용
>
> 데이터 카테고리:
> - 시각지능: 이미지, 영상 (불량 이미지, CCTV 등)
> - 언어지능: 텍스트, 음성 (제조 매뉴얼, 음성 명령 등)
> - 융합: 센서, 시계열 (설비 진동, 공정 데이터 등)
>
> **AI 학습이 목적이라면 AI 허브가 가장 적합**합니다. 라벨이 이미 있거든요.

### 2.4 Kaggle

> **Kaggle**은 Google이 운영하는 글로벌 데이터 과학 커뮤니티입니다.
>
> - 규모: 20만 개 이상 데이터셋, 1,600만 명 이상 사용자
> - 특징: 경진대회, 노트북(코드) 공유, 활발한 커뮤니티
>
> 핵심 기능:
> - **Datasets**: 무료 데이터셋 검색/다운로드
> - **Competitions**: 상금 경진대회
> - **Notebooks**: 다른 사람의 코드 바로 보고 실행
> - **Discussions**: 질문하면 전 세계 사람들이 답변
>
> 영어지만, 배움에는 국경이 없습니다!

### 2.5 포털 비교

> 포털 선택은 목적에 따라 다릅니다:
>
> | 목적 | 추천 포털 |
> |------|----------|
> | 제조 AI 모델 학습 | AI 허브 |
> | 경제/산업 분석 | 공공데이터포털, 통계청 |
> | ML 알고리즘 학습 | UCI, Kaggle |
> | 실시간 데이터 연동 | 공공데이터포털 (API) |
> | 글로벌 트렌드 파악 | Kaggle |
>
> 목적에 맞게 선택하세요.

---

## 대주제 3: 데이터셋 구조와 용어 (5분)

### 3.1 테이블 형태 데이터의 구조

> 데이터를 다루려면 용어를 정확히 알아야 합니다.
>
> 엑셀 표를 떠올려보세요. 가로줄은 **행(Row)**, 세로줄은 **열(Column)**입니다.
>
> 행을 다른 말로:
> - **레코드(Record)** - 하나의 데이터 항목
> - **샘플(Sample)** - ML에서 하나의 데이터
> - **관측치(Observation)** - 통계에서 하나의 측정값
>
> 열을 다른 말로:
> - **변수(Variable)** - 변할 수 있는 값
> - **특성(Feature)** - ML에서 입력 속성
> - **필드(Field)** - DB에서의 열
>
> 같은 개념인데 문맥에 따라 다른 용어를 씁니다.

### 3.2 독립변수와 종속변수

> AI/ML에서 가장 중요한 구분이 있습니다:
>
> **독립변수(X)**: 예측에 사용하는 입력 데이터
> - Feature, Input, 설명변수라고도 함
> - 예: 온도, 습도, 압력
>
> **종속변수(y)**: 예측하고 싶은 대상
> - Target, Label, 반응변수라고도 함
> - 예: 불량 여부, 생산량
>
> Python에서는 관례적으로 X, y라고 씁니다:
> ```python
> X = df[['온도', '습도', '압력']]  # 독립변수
> y = df['불량여부']                 # 종속변수
> ```

### 3.3 변수의 데이터 타입

> 변수는 타입에 따라 두 종류입니다:
>
> **수치형(Numerical)**:
> - 연속형: 온도 85.234°C처럼 무한히 많은 값
> - 이산형: 불량 개수 0, 1, 2처럼 정수값만
>
> **범주형(Categorical)**:
> - 명목형: 라인(A, B, C)처럼 순서 없음
> - 순서형: 등급(상>중>하)처럼 순서 있음
> - 이진형: 불량(0/1)처럼 두 가지만
>
> 타입에 따라 처리 방법이 달라집니다.

### 3.4 데이터 품질 용어

> 실제 데이터에는 항상 문제가 있습니다:
>
> - **결측치(Missing Value)**: 값이 없음 (NaN, null)
> - **이상치(Outlier)**: 비정상적으로 크거나 작은 값
> - **중복(Duplicate)**: 같은 레코드가 반복
> - **노이즈(Noise)**: 의미 없는 변동
> - **불균형(Imbalance)**: 클래스 비율이 안 맞음 (정상 95%, 불량 5%)
>
> 나중에 전처리 시간에 이것들을 어떻게 처리하는지 배웁니다.

### 3.5 데이터셋 분할

> ML에서는 데이터를 세 부분으로 나눕니다:
>
> - **훈련 세트(Training)**: 70% - 모델 학습용
> - **검증 세트(Validation)**: 15% - 하이퍼파라미터 튜닝용
> - **테스트 세트(Test)**: 15% - 최종 성능 평가용
>
> 중요한 건 **테스트 세트는 마지막에 딱 한 번만 사용**한다는 겁니다.
> 미리 보면 시험지 답안을 미리 보는 것과 같아요.

### 3.6 용어 정리표

> 자주 혼동되는 용어를 정리하면:
>
> | 한국어 | 영어 | 동의어 |
> |--------|------|--------|
> | 행 | Row | Record, Sample |
> | 열 | Column | Feature, Variable |
> | 독립변수 | X | Feature, Input |
> | 종속변수 | y | Target, Label |
> | 결측치 | Missing | NaN, Null |
>
> 나중에 논문이나 문서를 읽을 때 이 용어들이 나옵니다.

---

## 실습편 (15분)

### 실습 개요

> 이제 실제로 데이터를 받아봅시다!
>
> 오늘 실습은 세 단계입니다:
> 1. 공공데이터포털에서 CSV 다운로드 (5분)
> 2. AI 허브 또는 Kaggle에서 데이터 확보 (5분)
> 3. 다운로드한 데이터 구조 확인 (5분)
>
> 각자 컴퓨터에서 따라해 보세요.

### 실습 1: 공공데이터포털

> 먼저 공공데이터포털입니다.
>
> 1. 브라우저에서 **data.go.kr** 접속
> 2. 검색창에 "제조업" 또는 "스마트공장" 입력
> 3. 왼쪽 필터에서 "파일데이터" 선택
> 4. CSV 형식 데이터 골라서 다운로드
>
> 다운로드했으면 Jupyter에서 열어봅시다:
>
> ```python
> import pandas as pd
> df = pd.read_csv('파일명.csv', encoding='cp949')  # 한글 인코딩
> print(df.shape)
> print(df.head())
> ```
>
> encoding='cp949'는 공공데이터에서 자주 쓰는 한글 인코딩입니다. 안 되면 'utf-8'로 해보세요.

### 실습 2: AI 허브 / Kaggle

> AI 허브에서 제조 데이터를 받아봅시다.
>
> 1. **aihub.or.kr** 접속 → 로그인
> 2. 데이터 → 제조 카테고리 선택
> 3. "제조 공정 이상탐지" 같은 데이터셋 선택
> 4. 다운로드 신청 (승인 필요한 경우 있음)
>
> 시간이 부족하면 Kaggle에서 바로 받을 수 있습니다:
>
> 1. **kaggle.com** → Datasets
> 2. "predictive maintenance" 검색
> 3. Download 클릭
>
> ```python
> import zipfile
> with zipfile.ZipFile('archive.zip', 'r') as z:
>     z.extractall('./kaggle_data/')
>
> df = pd.read_csv('./kaggle_data/데이터파일.csv')
> ```

### 실습 3: 데이터 구조 확인

> 데이터를 받았으면 반드시 구조를 확인해야 합니다.
>
> ```python
> def check_dataset(df, name="Dataset"):
>     print("=" * 50)
>     print(f"📊 {name}")
>     print("=" * 50)
>
>     # 크기
>     print(f"\n1. 크기: {df.shape[0]:,}행 × {df.shape[1]}열")
>
>     # 데이터 타입
>     print(f"\n2. 데이터 타입:")
>     print(df.dtypes)
>
>     # 결측치
>     print(f"\n3. 결측치:")
>     print(df.isnull().sum())
>
>     # 기본 통계
>     print(f"\n4. 기본 통계:")
>     print(df.describe())
>
> check_dataset(df, "제조 데이터")
> ```
>
> 이 5가지는 데이터 받으면 무조건 확인하세요: shape, columns, dtypes, isnull, describe

---

## 마무리 (2분)

### 핵심 요약

> 오늘 배운 핵심을 정리하면:
>
> **1. 공공데이터의 정의와 활용**
> - 공공기관이 생성/관리하는 전자적 자료
> - 무료, 기계판독, 재이용 허용 원칙
> - AI 학습의 기반 데이터로 활용
>
> **2. 주요 데이터 포털의 특성**
> - 공공데이터포털: 정부 공식, API 풍부
> - AI 허브: AI 학습 특화, 고품질 라벨
> - Kaggle: 글로벌, 커뮤니티, 노트북
>
> **3. 데이터셋 구조와 용어**
> - 행/열, X/y, 수치형/범주형 구분
> - 결측치, 이상치, 불균형 등 품질 이슈

### 다음 차시 예고

> 다음 시간에는 **5차시: 기초 기술통계량과 탐색적 시각화**를 배웁니다.
>
> - 대표값: 평균, 중앙값, 최빈값
> - 퍼짐: 표준편차, 분산, 범위
> - 시각화: 히스토그램, 상자그림, 산점도
>
> 오늘 받은 데이터를 가지고 분석하고 그래프로 그려볼 겁니다.

### 마무리 인사

> 4차시 수업 마치겠습니다.
>
> 오늘 다룬 포털들은 앞으로 AI 프로젝트할 때 계속 사용하니까 꼭 기억해두세요.
> 특히 AI 허브는 제조 AI 데이터가 많으니까 회원가입 해두시면 좋습니다.
>
> 수고하셨습니다!

---

## 예상 질문 및 답변

### Q1. 공공데이터를 상업적으로 사용해도 되나요?

> 대부분의 공공데이터는 상업적 이용이 가능합니다. 다만 각 데이터의 라이선스를 확인해야 합니다.
>
> 공공누리 유형에 따라:
> - 1유형: 자유로운 이용 가능
> - 2유형: 출처 표시 필수
> - 3유형: 비영리 목적만
> - 4유형: 비영리 + 변경 금지
>
> 데이터 상세 페이지에서 라이선스를 꼭 확인하세요.

### Q2. API 키는 어떻게 발급받나요?

> 공공데이터포털 API 키 발급 절차:
> 1. data.go.kr 회원가입 및 로그인
> 2. 원하는 API 찾기
> 3. "활용신청" 버튼 클릭
> 4. 활용 목적 작성 후 신청
> 5. 대부분 즉시 승인 (일부는 검토 필요)
> 6. 마이페이지에서 API 키 확인
>
> 키는 노출되지 않게 관리하세요.

### Q3. 데이터 인코딩 오류가 나면 어떻게 하나요?

> 한글 데이터에서 자주 발생하는 문제입니다.
>
> ```python
> # 방법 1: cp949 (윈도우 한글)
> df = pd.read_csv('file.csv', encoding='cp949')
>
> # 방법 2: utf-8 (표준)
> df = pd.read_csv('file.csv', encoding='utf-8')
>
> # 방법 3: euc-kr
> df = pd.read_csv('file.csv', encoding='euc-kr')
>
> # 방법 4: 인코딩 자동 감지
> import chardet
> with open('file.csv', 'rb') as f:
>     result = chardet.detect(f.read())
> df = pd.read_csv('file.csv', encoding=result['encoding'])
> ```

### Q4. AI 허브 데이터 승인이 오래 걸리나요?

> 데이터셋에 따라 다릅니다:
> - 즉시 다운로드 가능: 대부분의 공개 데이터셋
> - 승인 필요 (1-3일): 민감 정보 포함 데이터
> - 협약 필요: 대규모 상업적 이용
>
> 수업 실습용이라면 즉시 다운로드 가능한 데이터셋을 선택하세요.

### Q5. Kaggle에서 한국 데이터를 찾을 수 있나요?

> Kaggle에서 "korea", "korean"으로 검색하면 한국 관련 데이터셋을 찾을 수 있습니다.
>
> 다만 대부분 영어권 데이터가 많아서, 한국 데이터는 공공데이터포털이나 AI 허브가 더 적합합니다.
>
> Kaggle은 글로벌 벤치마크 데이터, 알고리즘 학습용으로 활용하세요.
