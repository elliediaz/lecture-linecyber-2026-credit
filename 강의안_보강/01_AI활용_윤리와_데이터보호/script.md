# [1차시] AI 활용 윤리와 데이터 보호 - 강사 스크립트 (보강판)

## 강의 정보
- **차시**: 1차시 (25-30분)
- **유형**: 이론 중심 (실습 없음)
- **구성**: 이론 25-30분 (사례 중심, 토론형)
- **대상**: 비전공자, AI 입문자, 제조업 종사자

---

## 시간 배분

| 섹션 | 내용 | 시간 |
|------|------|------|
| 1 | 도입 및 동기부여 | 5분 |
| 2 | AI 윤리 4원칙 심화 | 10분 |
| 3 | 데이터 보안 사고 사례 | 8분 |
| 4 | AI 저작권과 법적 이슈 | 5분 |
| 5 | 정리 및 다음 차시 예고 | 2분 |

---

## 도입부 (5분)

### 인사 및 과정 소개 [1분]

> 안녕하세요, 여러분! '제조데이터를 활용한 AI 이해와 예측 모델 구축' 과정의 첫 번째 시간입니다.
>
> 저는 이 과정을 진행할 [강사명]입니다.
>
> 앞으로 27차시에 걸쳐 여러분과 함께 AI의 기초부터 실무 활용까지 배워볼 예정인데요,
> 오늘은 그 첫 번째 시간으로, 가장 중요하지만 종종 간과되는 **AI 윤리와 데이터 보호**에 대해 이야기하겠습니다.

### AI 시대의 도래 [1분]

> 여러분, 혹시 ChatGPT 써보신 분 계신가요?
>
> (손 들기 유도)
>
> 이제 AI는 우리 일상과 업무에 깊숙이 들어와 있습니다.
> 실제로 2024년 기준 국내 제조업 AI 도입률이 35%를 넘어섰다는 통계가 있어요.
>
> 불과 5년 전만 해도 10% 미만이었는데, 정말 빠르게 변화하고 있죠.

### 왜 윤리가 먼저인가? [1.5분]

> 그런데 왜 첫 시간에 코딩이나 기술이 아닌 **윤리**를 다룰까요?
>
> 2023년 삼성전자에서 일어난 일을 말씀드릴게요.
>
> 삼성 반도체 부서 직원들이 ChatGPT에 반도체 설비 관련 소스코드와 회의 내용을 입력했습니다.
> 업무 효율화를 위해서였죠.
>
> 그런데 이 정보가 외부 AI 서버로 전송되면서 기업 기밀 유출 우려가 발생했습니다.
> 결국 삼성은 ChatGPT 사용을 전면 금지했어요.
>
> 이 사례가 보여주듯이, AI를 잘못 사용하면 기술력보다 **윤리와 보안 문제**가 더 큰 리스크가 됩니다.
> 그래서 우리는 기술을 배우기 전에, 올바르게 사용하는 법부터 배워야 합니다.

### 학습목표 안내 [1.5분]

> 오늘 수업을 마치면 다음 네 가지를 할 수 있게 됩니다.
>
> **첫째**, AI 윤리의 4가지 핵심 원칙을 이해합니다.
> 공정성, 투명성, 책임성, 안전성 - 이 네 가지예요.
>
> **둘째**, 제조업 현장에서 발생할 수 있는 데이터 보안 위험을 파악합니다.
> 실제 사고 사례를 통해 배울 거예요.
>
> **셋째**, AI 생성물의 저작권과 법적 이슈를 인식합니다.
> AI가 만든 결과물, 누구 것일까요?
>
> **넷째**, 조직 내 AI 활용 시 윤리적 판단 기준을 수립합니다.
> 실무에서 바로 적용할 수 있는 체크리스트를 만들어볼 거예요.

---

## AI 윤리 4원칙 심화 (10분)

### 윤리 원칙 개요 [1분]

> AI 윤리는 크게 네 가지 원칙으로 나뉩니다.
>
> **공정성(Fairness)**, **투명성(Transparency)**, **책임성(Accountability)**, **안전성(Safety)**.
>
> 이 네 가지는 국제적으로 합의된 원칙이에요.
> EU, 미국, 한국 모두 이 원칙을 기반으로 AI 가이드라인을 만들었습니다.
>
> 각각을 자세히 살펴볼게요.

### 1. 공정성 (Fairness) [2분]

> 첫 번째, **공정성**입니다.
>
> AI는 특정 집단에 불이익을 주면 안 됩니다.
>
> **사례 1: 아마존 채용 AI**
>
> 2018년, 아마존은 AI 채용 시스템을 개발했습니다.
> 그런데 이 AI가 여성 지원자를 체계적으로 불리하게 평가했어요.
>
> 왜일까요? 학습 데이터가 과거 10년간 채용 데이터였는데,
> IT 업계 특성상 남성 합격자가 많았거든요.
> AI는 이 패턴을 학습해서 "남성이 더 적합하다"고 판단한 겁니다.
>
> 결국 아마존은 이 시스템을 폐기했습니다.
>
> **사례 2: 미국 범죄 예측 AI (COMPAS)**
>
> 미국 법원에서 사용하던 재범 예측 AI가 흑인에게 더 높은 재범 위험도를 부여했습니다.
> 역시 학습 데이터의 편향 때문이었어요.
>
> **제조업 시사점**
>
> 제조업에서도 마찬가지입니다.
> 품질 예측 AI가 특정 생산라인이나 작업자에게 불공정하게 작동하면
> 노동 분쟁으로 이어질 수 있어요.

### 2. 투명성 (Transparency) [2분]

> 두 번째, **투명성**입니다.
>
> AI의 결정 과정이 설명 가능해야 합니다.
>
> **사례 3: 금융 신용평가 AI**
>
> 은행에서 AI로 대출 심사를 합니다.
> 그런데 거절당한 고객이 "왜 거절됐나요?"라고 물으면
> "AI가 판단했습니다"라고만 답할 수 있을까요?
>
> EU의 GDPR은 이런 경우 **설명 요구권**을 보장합니다.
> AI가 왜 그런 결정을 내렸는지 설명할 수 있어야 해요.
>
> **블랙박스 vs 화이트박스**
>
> 딥러닝 모델은 종종 "블랙박스"라고 불립니다.
> 왜 그런 결과가 나왔는지 설명하기 어려워서요.
>
> 반면 의사결정나무 같은 모델은 "화이트박스"예요.
> 어떤 조건에서 어떤 결정을 내렸는지 명확히 보여줍니다.
>
> **제조업 시사점**
>
> 품질 불량 예측 AI가 "이 제품은 불량"이라고 판정했을 때,
> "왜 불량인가요?"라는 질문에 답할 수 있어야 합니다.
> 그래야 공정 개선에 활용할 수 있으니까요.

### 3. 책임성 (Accountability) [2분]

> 세 번째, **책임성**입니다.
>
> AI의 결과에 대해 누군가 책임져야 합니다.
>
> **사례 4: 자율주행 사고**
>
> 2018년 우버 자율주행차가 보행자를 치어 사망 사고가 발생했습니다.
> 이때 책임 소재가 문제가 됐어요.
>
> 운전석에 앉아있던 안전 담당자? 우버? AI 개발자? 차량 제조사?
>
> 결국 우버와 안전 담당자가 책임을 졌지만,
> AI 시대에는 책임 소재가 점점 복잡해집니다.
>
> **제조업 시사점**
>
> AI가 품질 OK 판정을 내렸는데 실제로 불량이었다면?
> 고객 클레임이 발생했을 때 책임은 누구에게 있을까요?
>
> AI 담당자? 품질 담당자? 경영진?
>
> 이런 책임 체계를 미리 정의해두어야 합니다.

### 4. 안전성 (Safety) [2분]

> 네 번째, **안전성**입니다.
>
> AI 시스템은 안전하게 작동해야 합니다.
>
> **사례 5: 테슬라 오토파일럿 사고**
>
> 테슬라 오토파일럿 기능 사용 중 여러 건의 사고가 발생했습니다.
> AI가 트럭을 하늘로 인식하는 등 오판단이 원인이었어요.
>
> **사례 6: 마이크로소프트 Tay 챗봇**
>
> 2016년 MS가 출시한 AI 챗봇 Tay가
> 24시간 만에 인종차별적, 혐오적 발언을 쏟아냈습니다.
> 악의적인 사용자들이 챗봇을 학습시킨 결과였어요.
>
> AI가 공격에 취약하면 큰 문제가 됩니다.
>
> **제조업 시사점**
>
> 제조 설비를 제어하는 AI가 오작동하면?
> 장비 손상은 물론, 작업자 안전까지 위협받을 수 있습니다.
> AI 시스템의 안전장치, 비상정지 체계가 필수입니다.

### 토론 1: 윤리적 딜레마 [1분]

> 자, 여기서 잠깐 생각해볼 문제입니다.
>
> **상황**: 여러분 회사의 AI가 특정 작업자의 불량률이 높다고 분석했습니다.
>
> **질문**: 이 정보를 어떻게 활용해야 할까요?
>
> 1. 해당 작업자에게 직접 알리고 교육을 권유한다
> 2. 관리자에게만 보고하고 조치를 맡긴다
> 3. 작업자 배치를 AI 추천에 따라 변경한다
> 4. 분석 결과를 활용하지 않는다
>
> (잠시 생각할 시간)
>
> 정답은 없습니다. 하지만 **투명성**과 **공정성**을 고려해야 해요.
> 작업자에게 설명 없이 불이익을 주면 안 됩니다.

---

## 데이터 보안 사고 사례 (8분)

### 데이터 보안의 중요성 [1분]

> 이제 데이터 보안으로 넘어가겠습니다.
>
> 제조업에서 데이터는 **기업의 핵심 자산**입니다.
>
> 공정 조건, 레시피, 품질 데이터...
> 이것들이 유출되면 경쟁력을 잃을 수 있어요.
>
> 실제 국내외 사고 사례를 살펴보겠습니다.

### 국내 사례 1: 삼성전자 ChatGPT 사건 (2023) [1.5분]

> 아까 잠깐 언급했던 삼성전자 사례입니다.
>
> **경과**:
> - 반도체 부서 직원들이 ChatGPT에 소스코드, 회의록 입력
> - 업무 효율화 목적이었으나 외부 서버로 데이터 전송
> - 기업 기밀 유출 우려 발생
>
> **결과**:
> - ChatGPT 등 생성형 AI 사용 전면 금지
> - 내부 AI 플랫폼 구축 추진
> - 보안 교육 강화
>
> **교훈**:
> 편리한 외부 AI 도구도 보안 관점에서 검토가 필요합니다.
> 특히 기밀 정보는 절대 입력하면 안 돼요.

### 국내 사례 2: LG에너지솔루션 배터리 기술 유출 (2021) [1min]

> LG에너지솔루션에서 핵심 배터리 기술이 유출된 사건입니다.
>
> **경과**:
> - 전직 연구원이 배터리 제조 기술 자료 유출
> - 중국 경쟁사로 이직하며 기술 반출
>
> **결과**:
> - 법적 소송 진행
> - 수천억 원 규모 피해 추정
>
> **교훈**:
> 내부자 위협이 가장 위험합니다.
> 퇴직 시 데이터 접근 권한 회수, 보안 서약 강화가 필요해요.

### 국내 사례 3: 현대차 협력사 해킹 (2022) [1min]

> 현대자동차 1차 협력사가 랜섬웨어 공격을 받은 사건입니다.
>
> **경과**:
> - 협력사 서버가 랜섬웨어에 감염
> - 현대차 생산 스케줄 등 데이터 유출 위험
> - 협력사를 통한 공급망 공격
>
> **교훈**:
> 우리 회사가 안전해도 협력사가 뚫리면 위험합니다.
> 공급망 전체의 보안 수준을 점검해야 해요.

### 해외 사례: 대만 TSMC 협력사 바이러스 감염 (2018) [1min]

> 세계 최대 반도체 파운드리 TSMC 사례입니다.
>
> **경과**:
> - 신규 장비 설치 시 바이러스 감염
> - 웨이퍼 팹 3곳 가동 중단
> - 3일간 생산 차질
>
> **피해**:
> - 약 2,500억 원 손실
> - 애플 아이폰 신제품 공급 지연
>
> **교훈**:
> 외부 장비, 소프트웨어 도입 시 보안 검증이 필수입니다.

### 데이터 보안 3대 영역 [1.5min]

> 이런 사고를 막으려면 세 가지 영역을 지켜야 합니다.
>
> **1. 개인정보 보호**
> - 작업자 정보, 건강 데이터
> - 개인정보보호법 준수
>
> **2. 기업 기밀 보호**
> - 공정 조건, 레시피
> - 설비 파라미터
> - 품질 데이터
>
> **3. 시스템 접근 통제**
> - AI 모델 접근 권한
> - 데이터베이스 접근 권한
> - 물리적 접근 통제
>
> 이 세 가지를 모두 관리해야 종합적인 보안이 됩니다.

### 토론 2: 보안 vs 편의성 [1min]

> **질문**: 보안을 강화하면 업무 효율이 떨어집니다.
> 어떻게 균형을 맞춰야 할까요?
>
> 예를 들어, 외부 AI 도구 사용 금지 vs 업무 효율화
>
> (잠시 생각할 시간)
>
> 정답은 **위험 수준에 따른 단계적 접근**입니다.
> - 기밀 데이터: 외부 AI 절대 금지
> - 일반 데이터: 가이드라인 내 사용 허용
> - 공개 데이터: 자유롭게 사용

---

## AI 저작권과 법적 이슈 (5분)

### AI 저작권 개요 [1min]

> 마지막으로 AI와 저작권에 대해 알아보겠습니다.
>
> AI가 생성한 결과물, 누구의 것일까요?
>
> - AI가 그린 그림?
> - AI가 쓴 코드?
> - AI가 분석한 보고서?
>
> 현재 법적으로는 명확하지 않습니다.
> 국가마다, 상황마다 다르게 해석되고 있어요.

### 저작권 분쟁 사례 [2min]

> **사례 1: Getty Images vs Stability AI**
>
> 2023년, 이미지 제공업체 Getty Images가
> AI 회사 Stability AI를 고소했습니다.
>
> Stable Diffusion이 Getty 이미지를 무단 학습했다는 이유예요.
> 현재도 소송이 진행 중입니다.
>
> **사례 2: GitHub Copilot 소송**
>
> 마이크로소프트의 코딩 AI Copilot이
> 오픈소스 코드를 무단 학습했다는 소송이 제기됐습니다.
>
> 오픈소스도 라이선스가 있는데, 이를 무시했다는 거죠.
>
> **사례 3: 뉴욕타임스 vs OpenAI**
>
> 2023년 말, 뉴욕타임스가 OpenAI를 고소했습니다.
> ChatGPT가 NYT 기사를 학습 데이터로 사용했다는 이유예요.

### 오픈소스 라이선스 [1.5min]

> 코드를 사용할 때 라이선스를 확인해야 합니다.
>
> **MIT License**: 가장 자유로움. 출처 표기만 하면 됨
>
> **Apache 2.0**: 특허권 관련 조항 포함
>
> **GPL**: 파생물도 오픈소스로 공개해야 함
>
> **상업용 라이선스**: 비용 지불 필요
>
> AI 학습에 사용할 데이터, 코드의 라이선스를 반드시 확인하세요.

### 실무 가이드라인 [0.5min]

> 실무에서 지켜야 할 것들을 정리하면:
>
> 1. 외부 AI에 기밀 정보 입력 금지
> 2. AI 생성물 사용 시 출처 명시
> 3. 학습 데이터 라이선스 확인
> 4. 사내 AI 활용 가이드라인 수립

---

## 정리 및 마무리 (2분)

### 핵심 요약 [1min]

> 오늘 배운 내용을 정리하겠습니다.
>
> **AI 윤리 4원칙**:
> - 공정성: 편향 없는 AI
> - 투명성: 설명 가능한 AI
> - 책임성: 책임질 수 있는 AI
> - 안전성: 안전하게 작동하는 AI
>
> **데이터 보안 3영역**:
> - 개인정보 보호
> - 기업 기밀 보호
> - 시스템 접근 통제
>
> **기억할 것**:
> - 외부 AI에 기밀 정보 입력 금지
> - 내부자 위협이 가장 위험
> - AI 결과물에도 저작권 이슈 있음

### 다음 차시 예고 [0.5min]

> 다음 시간에는 **Python 시작하기**를 배웁니다.
>
> Python 설치, 개발 환경 구축, 기본 문법까지
> 실습 위주로 진행할 예정이에요.
>
> 노트북 꼭 가져오세요!

### 마무리 인사 [0.5min]

> 오늘은 AI 활용의 첫 관문인 윤리와 보안을 배웠습니다.
>
> 기술도 중요하지만, 올바르게 사용하는 것이 더 중요합니다.
> 오늘 배운 내용을 실무에서 꼭 적용해보세요.
>
> 수고하셨습니다! 다음 시간에 만나요!

---

## 강의 노트

### 준비물
- PPT 슬라이드 (slides.md 기반 43장)
- 사례 뉴스 기사 링크 (선택)
- 토론 질문 카드 (선택)

### 주의사항
- 특정 기업을 비난하는 느낌 주지 않기 (교훈 중심)
- 법적 해석은 전문가 의견임을 언급
- 토론 시간 유동적으로 조절

### 예상 질문 및 답변

**Q1: 우리 회사에서 ChatGPT 써도 되나요?**
> 회사 보안 정책에 따라 다릅니다. 일반적으로 기밀 정보는 입력하면 안 되고, 일반 업무 보조 목적으로는 가이드라인 내에서 사용 가능할 수 있습니다. 먼저 정보보안팀에 문의하세요.

**Q2: AI가 내린 판단이 틀리면 누구 책임인가요?**
> 현재까지는 AI를 운영하는 조직과 담당자에게 책임이 있습니다. AI는 도구이므로, 최종 책임은 사용자에게 있어요.

**Q3: 오픈소스를 AI 학습에 써도 되나요?**
> 라이선스에 따라 다릅니다. MIT, Apache는 대부분 가능하지만, GPL은 주의가 필요해요. 상업적 사용 시 법무팀 확인을 권장합니다.

**Q4: 개인정보를 AI 학습에 사용해도 되나요?**
> 개인정보보호법에 따라 본인 동의가 필요합니다. 익명화/가명화 처리 후 사용하거나, 동의를 받아야 해요.

**Q5: AI 생성물에 저작권이 있나요?**
> 현재 한국 저작권법상 AI 생성물에는 저작권이 인정되지 않습니다. 하지만 사람이 창작적 기여를 했다면 보호받을 수 있어요.

### 시간 조절 팁

**시간 부족 시**:
- 토론 시간 축소
- 해외 사례 생략
- 저작권 부분 간략히

**시간 여유 시**:
- 토론 시간 확대
- 추가 사례 소개
- 질의응답 시간 확보

### 참고 자료

1. EU AI Act (2024)
2. 한국 인공지능 윤리 기준 (과기정통부, 2020)
3. OECD AI 원칙 (2019)
4. 개인정보보호법
5. 저작권법
