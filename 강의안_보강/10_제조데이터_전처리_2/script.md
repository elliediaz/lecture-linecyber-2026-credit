# [10차시] 제조 데이터 전처리 (2) - 강사 스크립트

## 수업 개요

| 항목 | 내용 |
|------|------|
| 차시 | 10차시 |
| 주제 | 제조 데이터 전처리 (2): 스케일링, 인코딩, 파이프라인 |
| 시간 | 30분 (이론 15분 + 실습 15분) |
| 목표 | 스케일링, 인코딩, sklearn Pipeline 활용 능력 습득 |

---

## 시간 배분

| 구분 | 시간 | 내용 |
|------|------|------|
| 도입 | 2분 | 복습, 학습 목표 |
| 이론 1 | 5분 | 스케일링(표준화, 정규화) |
| 이론 2 | 4분 | 범주형 데이터 인코딩 |
| 이론 3 | 4분 | sklearn 전처리 도구 (Pipeline) |
| 실습 | 13분 | 종합 전처리 실습 |
| 정리 | 2분 | 핵심 요약, 다음 차시 예고 |

---

## 도입 (2분)

### 슬라이드 1-2: 지난 시간 복습

**강사 멘트:**

> "안녕하세요, 10차시 수업을 시작하겠습니다."
>
> "지난 시간에 결측치와 이상치 처리를 배웠죠?"
>
> "오늘은 전처리의 두 번째 파트, **스케일링과 인코딩**을 배웁니다."
>
> "그리고 sklearn의 **Pipeline**을 활용해서 전처리를 자동화하는 방법도 배울 거예요."

### 슬라이드 3: 학습 목표

**강사 멘트:**

> "오늘의 학습 목표입니다."
>
> "첫째, **스케일링의 필요성**을 이해합니다. 왜 스케일링이 필요한지 알아볼게요."
>
> "둘째, **범주형 데이터 인코딩 방법**을 배웁니다. 문자를 숫자로 바꾸는 방법이에요."
>
> "셋째, **sklearn 전처리 도구**를 활용합니다. Pipeline으로 전처리를 자동화할 거예요."

---

## Part 1: 스케일링(정규화, 표준화)의 필요성 이해 (5분)

### 슬라이드 5-6: 왜 스케일링이 필요한가?

**강사 멘트:**

> "먼저 스케일링이 왜 필요한지 알아보겠습니다."
>
> "제조 데이터를 보면 변수마다 스케일이 다릅니다."
>
> "온도는 80~100 정도, 생산량은 1000~1500, 불량률은 0.01~0.05..."
>
> "이렇게 스케일 차이가 크면 무슨 문제가 생길까요?"
>
> *(학습자 답변 유도)*
>
> "네, **스케일이 큰 변수가 모델에 과도한 영향**을 미칩니다."
>
> "특히 거리 기반 알고리즘에서는 더 심각해요."

### 슬라이드 7-8: 스케일링이 필요한 알고리즘

**강사 멘트:**

> "모든 알고리즘에 스케일링이 필요한 건 아닙니다."
>
> "**필수인 알고리즘**: KNN, SVM, 신경망"
>
> "이들은 거리 계산이나 경사하강법을 사용하기 때문에 스케일에 민감해요."
>
> "반면 **트리 기반 알고리즘**(의사결정나무, 랜덤포레스트)은 스케일링 불필요합니다."
>
> "분기점을 기준으로 나누기만 하니까요."

### 슬라이드 9-11: 표준화 (StandardScaler)

**강사 멘트:**

> "첫 번째 스케일링 방법, **표준화**입니다."
>
> "Z-score 변환이라고도 하죠. 6차시에서 배운 그 Z-score예요."
>
> "공식은 **Z = (X - 평균) / 표준편차**"
>
> "변환 후에는 **평균이 0, 표준편차가 1**이 됩니다."
>
> ```python
> from sklearn.preprocessing import StandardScaler
> scaler = StandardScaler()
> X_scaled = scaler.fit_transform(X)
> ```

### 슬라이드 12-14: 정규화 (MinMaxScaler)

**강사 멘트:**

> "두 번째 방법, **정규화**입니다."
>
> "모든 값을 **0~1 범위**로 변환합니다."
>
> "공식은 **(X - 최솟값) / (최댓값 - 최솟값)**"
>
> "최솟값은 0이 되고, 최댓값은 1이 됩니다."
>
> "신경망에서 주로 사용해요. 활성화 함수가 0~1 범위에서 작동하기 때문이죠."

### 슬라이드 15-16: RobustScaler

**강사 멘트:**

> "이상치가 많은 데이터에는 **RobustScaler**를 사용합니다."
>
> "평균 대신 **중앙값**, 표준편차 대신 **IQR**을 사용해요."
>
> "이상치에 영향을 덜 받아서 '강건하다(Robust)'라고 부릅니다."

### 슬라이드 17: 스케일링 비교

**강사 멘트:**

> "세 가지 방법을 정리해볼게요."
>
> "**StandardScaler**: 일반적인 상황에서 먼저 시도"
>
> "**MinMaxScaler**: 신경망이나 이미지 데이터"
>
> "**RobustScaler**: 이상치가 많을 때"
>
> "대부분의 경우 StandardScaler부터 시작하면 됩니다."

### 슬라이드 18: 주의사항

**강사 멘트:**

> "스케일링에서 가장 중요한 주의사항!"
>
> "**fit은 학습 데이터만, transform은 모든 데이터에**"
>
> ```python
> # 올바른 방법
> scaler.fit(X_train)           # 학습 데이터로만 fit
> X_train_scaled = scaler.transform(X_train)
> X_test_scaled = scaler.transform(X_test)  # 같은 기준으로!
> ```
>
> "테스트 데이터에도 fit을 하면 **데이터 누수**가 발생합니다."
>
> "테스트 데이터 정보가 학습에 포함되면 안 돼요."

---

## Part 2: 범주형 데이터 인코딩 방법 (4분)

### 슬라이드 19-20: 범주형 데이터란?

**강사 멘트:**

> "이번에는 범주형 데이터 인코딩을 배우겠습니다."
>
> "범주형 데이터는 숫자가 아닌 데이터예요."
>
> "라인 A, B, C나 등급 상, 중, 하 같은 것들이죠."
>
> "문제는 대부분의 ML 모델이 **숫자만 입력**받을 수 있다는 거예요."
>
> "그래서 문자를 숫자로 바꿔야 합니다. 이걸 **인코딩**이라고 해요."

### 슬라이드 21: 범주형 데이터 유형

**강사 멘트:**

> "범주형 데이터는 두 가지 유형이 있어요."
>
> "**명목형**: 순서 관계가 없음. 라인 A, B, C는 순서가 없죠."
>
> "**순서형**: 순서 관계가 있음. 등급 상, 중, 하는 순서가 있어요."
>
> "유형에 따라 적절한 인코딩 방법이 다릅니다."

### 슬라이드 22-24: 레이블 인코딩

**강사 멘트:**

> "첫 번째 방법, **레이블 인코딩**입니다."
>
> "범주를 0, 1, 2 같은 숫자로 바꿔요."
>
> ```python
> from sklearn.preprocessing import LabelEncoder
> le = LabelEncoder()
> df['등급_숫자'] = le.fit_transform(df['등급'])
> ```
>
> "주의할 점! **알파벳순**으로 숫자가 부여됩니다."
>
> "그리고 숫자에 크기 의미가 생겨요. 0 < 1 < 2 이렇게요."
>
> "그래서 **순서가 있는 범주**에만 적합합니다."

### 슬라이드 25-27: 원-핫 인코딩

**강사 멘트:**

> "순서가 없는 범주에는 **원-핫 인코딩**을 사용합니다."
>
> "각 범주를 별도의 컬럼으로 만들어요."
>
> ```
> 라인  →  라인_A  라인_B  라인_C
>  A         1      0      0
>  B         0      1      0
>  C         0      0      1
> ```
>
> "해당 범주면 1, 아니면 0."
>
> "이렇게 하면 숫자 크기에 의미가 없어집니다. 공정한 표현이 되는 거죠."

### 슬라이드 28-29: pandas get_dummies

**강사 멘트:**

> "원-핫 인코딩을 가장 쉽게 하는 방법은 pandas의 **get_dummies**예요."
>
> ```python
> df_encoded = pd.get_dummies(df, columns=['라인'])
> ```
>
> "한 줄이면 끝납니다. 정말 간편해요."
>
> "**drop_first=True** 옵션을 쓰면 첫 번째 컬럼을 제거해서"
>
> "다중공선성 문제를 예방할 수 있어요."

### 슬라이드 30: 인코딩 선택 가이드

**강사 멘트:**

> "정리하면:"
>
> "**순서 있는 범주** (등급, 만족도) → 레이블 인코딩"
>
> "**순서 없는 범주** (라인, 색상) → 원-핫 인코딩"
>
> "**고유값이 많은 범주** (제품코드) → 빈도/타겟 인코딩"

---

## Part 3: sklearn 전처리 도구 활용 (4분)

### 슬라이드 31-32: sklearn 전처리 흐름

**강사 멘트:**

> "이제 sklearn의 전처리 도구를 배우겠습니다."
>
> "sklearn의 모든 전처리 클래스는 동일한 패턴을 따릅니다."
>
> "**fit()**: 학습 데이터에서 파라미터 학습"
>
> "**transform()**: 데이터 변환"
>
> "**fit_transform()**: fit + transform 한번에"

### 슬라이드 33-34: ColumnTransformer

**강사 멘트:**

> "실제 데이터는 수치형과 범주형이 섞여 있죠?"
>
> "**ColumnTransformer**를 사용하면 컬럼별로 다른 전처리를 적용할 수 있어요."
>
> ```python
> preprocessor = ColumnTransformer([
>     ('num', StandardScaler(), ['온도', '습도']),
>     ('cat', OneHotEncoder(), ['라인', '장비'])
> ])
> ```
>
> "수치형은 스케일링, 범주형은 인코딩. 한번에 처리됩니다."

### 슬라이드 35-36: Pipeline

**강사 멘트:**

> "**Pipeline**은 전처리와 모델을 연결해줍니다."
>
> "정말 강력한 도구예요."
>
> ```python
> pipe = Pipeline([
>     ('preprocessor', preprocessor),
>     ('classifier', LogisticRegression())
> ])
> pipe.fit(X_train, y_train)
> y_pred = pipe.predict(X_test)
> ```
>
> "fit 한 번이면 전처리와 모델 학습이 자동으로 됩니다."
>
> "predict 한 번이면 전처리와 예측이 자동으로 되고요."

### 슬라이드 37-38: Pipeline 장점

**강사 멘트:**

> "Pipeline의 장점이 뭘까요?"
>
> "첫째, **코드가 간결**해집니다."
>
> "둘째, **데이터 누수를 방지**합니다. fit/transform이 자동으로 분리되니까요."
>
> "셋째, **교차 검증**이 쉬워집니다."
>
> "실무에서는 거의 필수로 사용합니다."

### 슬라이드 39-40: 전처리 저장

**강사 멘트:**

> "마지막으로, 전처리 저장 방법입니다."
>
> ```python
> import joblib
> joblib.dump(pipe, 'model_pipeline.pkl')
> ```
>
> "전처리기와 모델을 함께 저장해야 합니다."
>
> "나중에 새 데이터가 들어오면 같은 전처리를 적용해야 하니까요."
>
> "Pipeline으로 묶어서 저장하면 편리해요."

---

## 실습 (13분)

### 슬라이드 41-42: 실습 안내

**강사 멘트:**

> "이제 실습을 해보겠습니다."
>
> "제조 센서 데이터를 사용해서:"
> 1. "StandardScaler, MinMaxScaler 적용"
> 2. "LabelEncoder, OneHotEncoder 적용"
> 3. "Pipeline 구성"
>
> "순서대로 따라해보세요."

### 실습 1: 데이터 준비 (2분)

**강사 멘트:**

> "먼저 실습용 데이터를 생성합니다."
>
> ```python
> import numpy as np
> import pandas as pd
>
> np.random.seed(42)
> df = pd.DataFrame({
>     '온도': np.random.normal(85, 5, 200),
>     '습도': np.random.normal(60, 10, 200),
>     '생산량': np.random.normal(1200, 50, 200),
>     '라인': np.random.choice(['A', 'B', 'C'], 200),
>     '등급': np.random.choice(['상', '중', '하'], 200),
>     '불량': np.random.choice([0, 1], 200, p=[0.8, 0.2])
> })
> ```

### 실습 2: 스케일 차이 확인 (2분)

**강사 멘트:**

> "변수별 범위를 확인해봅시다."
>
> ```python
> for col in ['온도', '습도', '생산량']:
>     print(f"{col}: {df[col].min():.1f} ~ {df[col].max():.1f}")
> ```
>
> "생산량의 범위가 다른 변수보다 훨씬 크죠?"
>
> "이 상태로 모델을 학습하면 생산량이 지배적인 영향을 미칠 거예요."

### 실습 3: 표준화 적용 (2분)

**강사 멘트:**

> "StandardScaler를 적용해봅시다."
>
> ```python
> from sklearn.preprocessing import StandardScaler
>
> scaler = StandardScaler()
> X_scaled = scaler.fit_transform(df[['온도', '습도', '생산량']])
> df_scaled = pd.DataFrame(X_scaled, columns=['온도', '습도', '생산량'])
>
> print(df_scaled.describe().round(3))
> ```
>
> "평균이 거의 0, 표준편차가 1에 가깝게 됐죠?"

### 실습 4: 인코딩 적용 (3분)

**강사 멘트:**

> "범주형 데이터를 인코딩해봅시다."
>
> "등급은 순서가 있으니까 레이블 인코딩:"
>
> ```python
> from sklearn.preprocessing import LabelEncoder
> le = LabelEncoder()
> df['등급_encoded'] = le.fit_transform(df['등급'])
> ```
>
> "라인은 순서가 없으니까 원-핫 인코딩:"
>
> ```python
> df_encoded = pd.get_dummies(df, columns=['라인'])
> ```

### 실습 5: Pipeline 구성 (4분)

**강사 멘트:**

> "이제 Pipeline을 구성해봅시다."
>
> ```python
> from sklearn.compose import ColumnTransformer
> from sklearn.pipeline import Pipeline
> from sklearn.linear_model import LogisticRegression
>
> preprocessor = ColumnTransformer([
>     ('num', StandardScaler(), ['온도', '습도', '생산량']),
>     ('cat', OneHotEncoder(), ['라인'])
> ])
>
> pipe = Pipeline([
>     ('preprocessor', preprocessor),
>     ('classifier', LogisticRegression())
> ])
>
> pipe.fit(X_train, y_train)
> print(f"정확도: {pipe.score(X_test, y_test):.3f}")
> ```

---

## 정리 (2분)

### 슬라이드 48-49: 핵심 요약

**강사 멘트:**

> "오늘 배운 내용을 정리하겠습니다."
>
> "**스케일링**:"
> - "StandardScaler: 평균0, 표준편차1 (일반적)"
> - "MinMaxScaler: 0~1 범위 (신경망)"
> - "RobustScaler: 이상치에 강건"
>
> "**인코딩**:"
> - "LabelEncoder: 순서 있는 범주"
> - "OneHotEncoder / get_dummies: 순서 없는 범주"
>
> "**Pipeline**:"
> - "전처리 + 모델 자동화"
> - "데이터 누수 방지"

### 슬라이드 50: 다음 차시 예고

**강사 멘트:**

> "다음 10차시에서는 **EDA 종합**을 배웁니다."
>
> "1~10차시에서 배운 모든 내용을 활용해서"
>
> "실제 제조 데이터를 처음부터 끝까지 분석해볼 거예요."
>
> "오늘 수고하셨습니다!"

---

## 예상 질문 및 답변

### Q1: StandardScaler와 MinMaxScaler 중 어떤 걸 써야 하나요?

**A:** 상황에 따라 다릅니다:
- **일반적인 경우**: StandardScaler를 먼저 시도
- **신경망**: MinMaxScaler (활성화 함수가 0~1에서 잘 작동)
- **이상치가 있는 경우**: RobustScaler

확실하지 않으면 둘 다 해보고 성능 비교하는 것도 방법입니다.

### Q2: 왜 테스트 데이터에 fit을 하면 안 되나요?

**A:** **데이터 누수(Data Leakage)** 때문입니다:
- fit은 데이터의 통계(평균, 표준편차 등)를 학습
- 테스트 데이터에 fit하면 테스트 정보가 학습에 포함
- 실제 운영 환경에서는 미래 데이터 정보를 알 수 없음
- 과적합된 결과를 얻을 수 있음

항상 학습 데이터로만 fit하고, 같은 기준으로 테스트 데이터를 transform하세요.

### Q3: 원-핫 인코딩에서 drop_first는 언제 쓰나요?

**A:** **다중공선성 방지**를 위해 사용합니다:
- 원-핫 인코딩하면 컬럼 합이 항상 1 (완벽한 선형 관계)
- 선형 모델에서 계수 추정이 불안정해질 수 있음
- drop_first=True로 하나를 제거하면 해결

트리 기반 모델에서는 상관없지만, 선형 모델에서는 권장합니다.

### Q4: Pipeline 없이 그냥 순서대로 해도 되지 않나요?

**A:** 작동은 하지만 Pipeline 사용을 권장합니다:
1. **실수 방지**: transform 빠뜨리기 쉬움
2. **데이터 누수 방지**: 교차 검증 시 자동 처리
3. **배포 용이**: 하나의 객체로 저장/로드
4. **코드 가독성**: 전체 흐름을 한눈에 파악

실무에서는 거의 필수로 사용합니다.

### Q5: 범주형 변수의 고유값이 100개 이상이면 어떻게 하나요?

**A:** 원-핫 인코딩 시 100개 컬럼이 생겨 비효율적입니다. 대안:
1. **빈도 인코딩**: 범주별 출현 빈도로 대체
2. **타겟 인코딩**: 범주별 타겟 평균으로 대체
3. **그룹화**: 상위 N개만 유지, 나머지 '기타'

```python
# 빈도 인코딩 예시
freq = df['제품코드'].value_counts(normalize=True)
df['제품코드_freq'] = df['제품코드'].map(freq)
```

---

## 강사 노트

### 시연 시 주의사항

1. **스케일링 전후 비교**: 히스토그램으로 분포 변화 시각화
2. **데이터 누수 설명**: fit/transform 순서 강조
3. **Pipeline 장점**: 코드 비교로 간결함 보여주기

### 학습자 수준별 가이드

- **초급자**: StandardScaler, get_dummies 위주
- **중급자**: Pipeline 구성까지
- **고급자**: ColumnTransformer + GridSearchCV 조합

### 추가 참고 자료

- sklearn 전처리 문서: https://scikit-learn.org/stable/modules/preprocessing.html
- Pipeline 가이드: https://scikit-learn.org/stable/modules/compose.html
