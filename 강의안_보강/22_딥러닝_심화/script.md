# [22차시] 딥러닝 심화 - 강사 스크립트

## 수업 개요

| 항목 | 내용 |
|------|------|
| 차시 | 22차시 |
| 주제 | 딥러닝 심화 (CNN, RNN, 고급 아키텍처) |
| 시간 | 30분 (이론 28분 + 정리 2분) |
| 학습 목표 | CNN, RNN 구조 이해, 고급 아키텍처 개념 파악 |

---

## 학습 목표

1. CNN(합성곱 신경망)의 구조와 원리를 이해한다
2. RNN(순환 신경망)의 특징과 활용을 파악한다
3. 고급 아키텍처의 개념을 살펴본다

---

## 시간 배분

| 구간 | 시간 | 내용 |
|------|------|------|
| 도입 | 2분 | 복습 및 학습목표 |
| 대주제 1 | 10분 | CNN (합성곱 신경망) |
| 대주제 2 | 10분 | RNN (순환 신경망) |
| 대주제 3 | 6분 | 고급 아키텍처 |
| 정리 | 2분 | 요약 및 다음 차시 예고 |

---

## 상세 스크립트

### 도입부 (2분)

#### 슬라이드 1-3: 복습

> "지난 시간에 Keras로 MLP를 구현했습니다. 센서 데이터로 품질을 예측하는 모델을 만들었죠."

> "오늘은 MLP를 넘어서 다양한 딥러닝 아키텍처를 살펴봅니다. CNN, RNN, 그리고 최신 기술들을 개념 수준에서 이해합니다."

---

#### 슬라이드 4-5: 학습 목표

> "오늘의 학습 목표는 세 가지입니다. CNN의 구조와 원리, RNN의 특징과 활용, 그리고 고급 아키텍처의 개념을 파악합니다."

> "실습보다는 개념 이해에 집중합니다. 필요할 때 더 깊이 공부할 수 있는 기반을 만드는 거예요."

---

### 대주제 1: CNN (합성곱 신경망) (10분)

#### 슬라이드 6-8: 왜 CNN인가

> "먼저 CNN입니다. 왜 이미지 처리에 CNN이 필요할까요?"

> "MLP로 28×28 이미지를 처리하려면 784개 입력이 필요합니다. 첫 은닉층에 128개 노드만 있어도 파라미터가 10만 개예요."

> "더 큰 문제는 위치 정보입니다. 이미지에서 고양이가 왼쪽에 있든 오른쪽에 있든 같은 고양이인데, MLP는 이걸 다르게 봅니다."

> "CNN은 필터를 사용해서 지역적 패턴을 학습합니다. 파라미터도 적고, 위치에 상관없이 같은 패턴을 인식합니다."

---

#### 슬라이드 9-12: 합성곱 연산

> "합성곱 연산을 보겠습니다. 작은 필터가 이미지 위를 움직이면서 내적을 계산합니다."

> "3×3 필터가 있으면 이미지의 3×3 영역과 곱해서 하나의 숫자를 만듭니다. 이걸 전체 이미지에 반복하면 특징 맵이 나옵니다."

> "중요한 건 필터 값을 우리가 정하는 게 아니라 학습된다는 거예요. CNN이 스스로 수평 엣지, 수직 엣지 같은 패턴을 찾아냅니다."

---

#### 슬라이드 13-15: 풀링

> "풀링은 크기를 줄이는 연산입니다. 2×2 맥스 풀링이면 4개 중 최댓값만 남겨요."

> "왜 필요할까요? 첫째, 계산량을 줄입니다. 둘째, 위치에 덜 민감해집니다. 패턴이 조금 이동해도 같은 결과가 나와요."

---

#### 슬라이드 16-18: Keras에서 CNN

```python
model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    MaxPooling2D((2, 2)),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Flatten(),
    Dense(128, activation='relu'),
    Dense(10, activation='softmax')
])
```

> "Keras에서는 Conv2D와 MaxPooling2D를 사용합니다. 합성곱과 풀링을 반복하고, Flatten으로 펴서 Dense로 분류합니다."

---

#### 슬라이드 19-21: CNN 활용

> "제조업에서 CNN은 불량 이미지 검출에 많이 쓰입니다. 표면 결함, 조립 불량 같은 걸 자동으로 찾아내요."

> "의료에서는 X-ray, MRI 분석에, 자율주행에서는 객체 인식에 쓰입니다."

---

#### 슬라이드 22-24: 전이학습

> "CNN을 처음부터 학습하려면 수만 장의 이미지가 필요합니다. 하지만 전이학습을 쓰면 수백 장으로도 가능해요."

> "ImageNet으로 미리 학습된 VGG16, ResNet 같은 모델을 가져와서 마지막 분류기만 새로 학습합니다. 기존 필터는 그대로 쓰는 거예요."

> "제조업처럼 데이터가 적은 환경에서 특히 유용합니다."

---

### 대주제 2: RNN (순환 신경망) (10분)

#### 슬라이드 25-27: 왜 RNN인가

> "RNN은 순차 데이터를 위한 신경망입니다. 시계열, 텍스트처럼 순서가 중요한 데이터요."

> "MLP의 문제는 입력이 고정 크기고 순서 정보를 무시한다는 겁니다. '나는 밥을 먹었다'와 '밥을 나는 먹었다'를 구분 못 해요."

> "RNN은 이전 상태를 기억합니다. 과거 정보가 현재 예측에 영향을 줍니다."

---

#### 슬라이드 28-30: RNN 구조

> "RNN의 핵심은 은닉 상태입니다. 시점 t에서 계산된 은닉 상태가 시점 t+1로 전달됩니다."

> "수식으로 보면 h_t = tanh(W × h_{t-1} + U × x_t). 이전 은닉 상태와 현재 입력을 조합해서 새 은닉 상태를 만듭니다."

> "이렇게 하면 시퀀스 전체의 문맥을 누적할 수 있어요."

---

#### 슬라이드 31-33: LSTM

> "기본 RNN은 긴 시퀀스에서 문제가 있습니다. 먼 과거 정보가 사라지는 기울기 소실이 일어나요."

> "LSTM은 이걸 해결합니다. 셀 상태라는 장기 기억 저장소를 두고, 게이트로 정보 흐름을 제어합니다."

> "forget gate는 버릴 정보, input gate는 저장할 정보, output gate는 출력할 정보를 결정합니다."

---

#### 슬라이드 34-36: Keras에서 LSTM

```python
model = Sequential([
    LSTM(64, input_shape=(24, 5), return_sequences=True),
    Dropout(0.2),
    LSTM(32),
    Dense(1)
])
```

> "Keras에서는 LSTM 층을 사용합니다. input_shape는 (시퀀스 길이, 특성 수)입니다."

> "return_sequences=True면 모든 시점의 출력을 반환하고, False면 마지막 시점만 반환합니다. LSTM을 쌓으려면 True가 필요해요."

---

#### 슬라이드 37-39: GRU

> "GRU는 LSTM의 간소화 버전입니다. 게이트가 2개로 줄었어요."

> "파라미터가 적어서 학습이 빠르고, 성능은 LSTM과 비슷합니다. 데이터가 적으면 GRU가 나을 수 있어요."

---

#### 슬라이드 40-42: RNN 활용

> "제조업에서 RNN/LSTM은 시계열 예측에 쓰입니다. 과거 센서 데이터로 미래 생산량을 예측하거나, 설비 이상을 탐지합니다."

> "18차시에서 RandomForest로 했던 시계열 예측을 LSTM으로 하면 더 복잡한 패턴을 잡을 수 있어요."

---

### 대주제 3: 고급 아키텍처 (6분)

#### 슬라이드 43-45: ResNet

> "ResNet은 2015년에 나온 혁신적인 아키텍처입니다. Skip Connection이라는 기법을 도입했어요."

> "깊은 네트워크는 학습하기 어렵습니다. 기울기가 사라지거든요. ResNet은 입력을 출력에 직접 더해서 기울기가 직접 흐르게 합니다."

> "이 덕분에 152층짜리 네트워크도 학습할 수 있게 됐습니다."

---

#### 슬라이드 46-48: Transformer와 Attention

> "Transformer는 2017년에 나와서 자연어 처리를 혁신했습니다. GPT, BERT의 기반이에요."

> "핵심은 Attention 메커니즘입니다. '나는 사과를 먹었다'에서 '먹었다'를 예측할 때 '사과를'에 더 집중하는 거예요."

> "RNN 없이 Attention만으로 시퀀스를 처리하니까 병렬 처리가 가능해서 학습이 빠릅니다."

---

#### 슬라이드 49-51: 대형 언어 모델과 생성 모델

> "GPT-3, GPT-4 같은 대형 언어 모델은 수천억 개의 파라미터를 가집니다. 텍스트 생성, 번역, 요약 등을 합니다."

> "이미지 생성에서는 Diffusion 모델이 있습니다. DALL-E, Stable Diffusion 같은 거예요. 노이즈에서 이미지를 만들어냅니다."

> "제조업에서는 아직 직접 적용하기 어렵지만, 설비 로그 분석이나 합성 데이터 생성에 활용 가능성이 있습니다."

---

#### 슬라이드 52-54: 아키텍처 선택 가이드

> "정리하면, 데이터 유형에 따라 아키텍처를 선택합니다."

> "정형 데이터(테이블)는 MLP나 ML, 이미지는 CNN, 시계열은 LSTM, 텍스트는 Transformer가 적합합니다."

> "처음부터 복잡한 모델을 쓸 필요는 없어요. 간단한 것부터 시작해서 필요할 때 복잡한 모델로 넘어가세요."

---

### 정리 (2분)

#### 슬라이드 55-56: 핵심 정리

> "오늘 배운 내용을 정리합니다."

> "**CNN**: 이미지 처리에 특화. 합성곱과 풀링으로 지역 패턴 추출. 전이학습으로 적은 데이터에도 적용 가능."

> "**RNN/LSTM**: 시계열, 순차 데이터 처리. 은닉 상태로 문맥 유지. LSTM은 장기 의존성 학습 가능."

> "**고급 아키텍처**: ResNet의 Skip Connection, Transformer의 Attention. 계속 발전 중."

---

#### 슬라이드 57-58: 다음 차시 예고

> "다음 시간에는 모델 해석을 배웁니다. 모델이 왜 그런 예측을 했는지 이해하는 방법이에요. Feature Importance, Permutation Importance를 다룹니다."

> "오늘 수업 마무리합니다. 수고하셨습니다!"

---

## 예상 질문 및 답변

### Q1: CNN을 언제 써야 하나요?

> "이미지나 2D 패턴 데이터가 있을 때 씁니다. 제조업에서는 불량 이미지 검출, 표면 검사 같은 비전 문제에 적합해요. 정형 데이터(센서 값)에는 MLP가 낫습니다."

### Q2: LSTM과 GRU 중 뭘 써야 하나요?

> "성능 차이는 크지 않습니다. 데이터가 충분하면 LSTM, 적으면 GRU를 추천합니다. 둘 다 실험해보고 검증 성능이 좋은 걸 선택하세요."

### Q3: Transformer가 RNN을 대체했나요?

> "자연어 처리에서는 대부분 Transformer를 씁니다. 하지만 시계열 예측에서는 아직 LSTM도 많이 씁니다. 문제에 따라 다릅니다."

### Q4: 전이학습은 어떻게 하나요?

> "Keras에서 VGG16(weights='imagenet')으로 사전 학습된 모델을 불러오고, include_top=False로 분류기를 제외합니다. 그 위에 새 분류기를 추가하고 학습시킵니다."

### Q5: 우리 공장에서 CNN을 쓸 수 있을까요?

> "제품 이미지를 촬영할 수 있고, 정상/불량 라벨이 있으면 가능합니다. 전이학습을 쓰면 수백 장으로도 시작할 수 있어요. 데이터가 더 쌓이면 성능이 좋아집니다."

### Q6: 딥러닝이 항상 ML보다 좋나요?

> "아닙니다. 정형 데이터에서는 RandomForest, XGBoost가 더 좋을 때도 많아요. 딥러닝은 이미지, 텍스트, 대용량 데이터에서 강점을 보입니다. 문제에 맞게 선택하세요."

---

## 참고 자료

### 공식 문서
- [Keras CNN 튜토리얼](https://keras.io/examples/vision/)
- [Keras RNN 튜토리얼](https://keras.io/examples/nlp/)
- [TensorFlow 전이학습](https://www.tensorflow.org/tutorials/images/transfer_learning)

### 추천 자료
- CS231n: CNN for Visual Recognition (Stanford)
- CS224n: NLP with Deep Learning (Stanford)
- 3Blue1Brown - Neural Networks (YouTube)

### 관련 차시
- 20차시: 딥러닝 실습 - MLP
- 22차시: 모델 해석과 변수별 영향력 분석

---

## 체크리스트

수업 전:
- [ ] CNN, RNN 핵심 개념 정리
- [ ] 제조업 적용 사례 준비
- [ ] 아키텍처 비교표 준비

수업 중:
- [ ] 합성곱 연산 직관적 설명
- [ ] LSTM 게이트 간단히 설명
- [ ] 아키텍처 선택 기준 강조
- [ ] 실습보다 개념 이해에 집중

수업 후:
- [ ] 참고 자료 링크 공유
- [ ] 모델 해석 예고
